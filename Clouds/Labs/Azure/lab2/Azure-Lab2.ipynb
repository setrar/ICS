{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318597fc-bdd8-47cb-8c4e-465c86f617ea",
   "metadata": {},
   "source": [
    "$$\\Huge{\\textbf{CLOUDS Assignment 2}}$$\n",
    "\n",
    "This assignment is a series of labs using the Azure cloud. In the last assignment, you covered\n",
    "material from lectures 1 and 2. In this assignment, you will carry out a series of tasks that cover\n",
    "lectures 3 and 4.\n",
    "\n",
    "**NOTE: All the code you write in this assignment should be stored in a git repository. I\n",
    "suggest you one repo for all your assignments with separate folders per assignment. You\n",
    "can use gitlab.eurecom.fr. You will need to submit your gitlab link.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c413532-7146-4f8a-9f67-0e7cadbd06b0",
   "metadata": {},
   "source": [
    "# 1. Create a microservice that does numerical integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a081d-f4bd-4eae-adfb-b5a63342426d",
   "metadata": {},
   "source": [
    "1. We saw how to do numerical integration in the class. You first task is to create a program that can\n",
    "do numerical integration with the function abs(sin(x)). Given an interval lower and upper, your code\n",
    "should break up the interval into $N$ subintervals, compute the area of the rectangle at each\n",
    "subinterval and add them all up. For example, if you give as input 0 and 3.14159 (which is\n",
    "approximately $\\pi$), you should get $1.99…$ which is close to $2$ ($\\int \\sin 𝑥 = − \\cos 𝑥$, ignoring aspects of continuity, which when evaluated in the range gives you $- \\cos \\pi + \\cos 0 = 2$). Your program should loop and repeatedly compute numerical integral for $N = 10, 100, 100, 1000, 10k, 100k, 1M$. You will\n",
    "end up getting 7 values, one for each value of N. You will see that as N increases, result converges to 2. This will also make the integral computation time consuming which will be useful for load testing later.\n",
    "\n",
    "2. Now take the python program from above and convert it into a microservice. Read section 19.1 if\n",
    "you want to know what a microservice is if you have not read it already. You can use any\n",
    "framework you want (python (flask), javascript (nodejs), C# (.net)). I recommend python & flask.\n",
    "The microservice, when run, will take the lower and upper as arguments. These could be URL\n",
    "route or parameters (http://localhost:5000/numericalintegralservice/0/3.14159). What you are\n",
    "doing is instead of invoking the program via python interpreter, you are invoking it via a HTTP\n",
    "call.\n",
    "\n",
    "3. Test your microservice locally on your computer. If you cant use a local computer, you can get a\n",
    "VM from Azure and do your development there. Use curl or a browser to verify that it works.\n",
    "Next, load test your microservice with Locust (https://locust.io/). The goal is to create many\n",
    "clients that all invoke the service over HTTP simulatenously. Test with 1 client. If it works, test\n",
    "with multiple clients. Here’s a reference guide to run locust without a UI1\n",
    "(https://docs.locust.io/en/stable/running-without-web-ui.html#running-without-web-ui). You can\n",
    "use a simple locustfile.py like the following:\n",
    "```python\n",
    "import time\n",
    "from locust import HttpUser, task, between\n",
    "\n",
    "class QuickstartUser(HttpUser):\n",
    "    @task\n",
    "    def hello_world(self):\n",
    "        self.client.get(\"/integral/0/3.14159\")\n",
    "```\n",
    "\n",
    "4. **For your deliverable, run Locust for 3 minutes and save its output. You will have stable\n",
    "performance. You will use the output to plot a graph later.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a07388-0521-44ef-8b27-eae5ad5c6ec9",
   "metadata": {},
   "source": [
    "# 2. Improving availability with scalesets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d025722-9b47-4424-a028-e62c02e74a27",
   "metadata": {},
   "source": [
    "1. Read AzureMol book chapter 5 to know about Networking (vnics mainly and ssh-agent). Read\n",
    "chapter 6 to know about ARM templates, chapte 7 about availability, and chapter 8 about load\n",
    "balancing. You don’t have to do the exercises if you don’t want to. Now read chapter 9 with\n",
    "particular focus on VM scaling. You will see how all the above come together.\n",
    "2. Create a VM scaleset with 2 VMs. Manually configure both VMs so that you the microservice you\n",
    "built in part 1 is deployed on both VMs. Note that the VMs will be behind a load balancer this\n",
    "time. You will be tunneling (all of this is explained in the book). Here’s a reference for a quick and\n",
    "dirty way to configure your VMs to deploy a flask app with NGINX\n",
    "(https://krishansubudhi.github.io/webapp/2018/12/01/flaskwebapp.html)\n",
    "3. Use your development VM or your computer to now perform a load testing on the scale set you\n",
    "just deployed. While the load tester is running, check Azure Insights or VM directly to see which\n",
    "VM is serving the load. Note the latencies reported by your load tester also.\n",
    "4. While the load tester is running, manually shutdown the VM that is servicing the load. Notice\n",
    "what happens with the load tester – does throughput drop? Does it go back up again? Notice\n",
    "how load balancer redirects load to the other VM.\n",
    "5. **NOTE: You will need to run Locust for 3 minutes and save its output. In the middle of this\n",
    "run, you will turn off the VM. You will use the output to plot a graph later.**\n",
    "6. Delete the scaleset and free up all resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81786599-e671-41ba-9a39-93eaf0f33ae5",
   "metadata": {},
   "source": [
    "# 3. Scaling with Azure webapps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89840501-d11e-4923-ab8c-8bb7f45c5ebc",
   "metadata": {},
   "source": [
    "1. In part 2, you improved availability with scaleset. If you create a custom VM image, you can\n",
    "configure Azure to automatically scale your VM scalset. But that’s a lot of work in creating the\n",
    "image. In this task, you will use Azure Webapps to scale your microservice\n",
    "2. First, read the part in Chapter 9 about scaling webapps (9,3 and 9.4). Deploy your microservice on\n",
    "Azure Webapps. Azure learn is a fantastic resource for a quick tip on how to deploy an\n",
    "microservice to webapps (https://learn.microsoft.com/en-us/azure/app-service/quickstart-python)\n",
    "3. Configure your webapp to do auto-scaling. Start with 1 instance and scale to 3. You can use CPU\n",
    "load as the metric and you can use avg CPU usage is > 50% as a rule. You should use 1 minute\n",
    "monitoring time window so that azure reacts quickly to changes.\n",
    "4. Now perform a load testing with locust again, this time with the webapp URL. Notice how\n",
    "performance improves after each minute. Go on Azure website and use AppInsights to see how\n",
    "many instances were deployed, CPU usage etc.\n",
    "5. **Run Locust for 3 minutes and save the output. Stop the webapp (don’t deallocate – You will\n",
    "enable it before you submit the report)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1627517-a7ad-4485-8346-1a07b3b55578",
   "metadata": {},
   "source": [
    "# 4. Scaling microservice with functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0fdab-8df2-452b-a70f-e0d3f963af54",
   "metadata": {},
   "source": [
    "1. So far you did scaling with webapps. You saw how much easier it is to autoscale. For the next two\n",
    "parts, you will explore Azure functions. In this part, you will deploy the numericalintegral code as a\n",
    "function and you will perform autoscaling as before. This time you will see how functions make it\n",
    "even easier to scale apps.\n",
    "2. First, read Azuremol chapter 21 about serverless and functions. You don’t have to do the labs.\n",
    "An even better resource is azure learn. Go ahead and read: https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python?pivots=python-mode-configuration.\n",
    "It is way easier to develop functions with VSCode. So follow the steps in the link above,\n",
    "implement a simple function on VScode, test it locally, deploy it, and test it on Azure.\n",
    "3. Now, turn your numericalintegrap app into a function based on what you learned in the previous\n",
    "step. Deploy the function on azure. Configure the function so that it scales automatically (you can\n",
    "leave max instances to 200).\n",
    "4. Now, repeat the locust load testing with the function endpoint. As locust runs, you can use\n",
    "AppInsights to see how functions are scaling and your locust performance is improving.\n",
    "5. **Run Locust for 3 minutes and save the output. Stop the function. You will enable it before you submit the report)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370030b6-c97c-47f0-b430-9efa656404be",
   "metadata": {},
   "source": [
    "# 5. Implement MapReduce using Azure Durable functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53819bf9-1f99-4759-a6c4-35fd3efef357",
   "metadata": {},
   "source": [
    "- For your final task, you will be implementing MapReduce and using it to do word count on a bunch\n",
    "of documents!! In doing so, you will learn about Azure Durable function and Azure Blob store.\n",
    "- First, read Durable functions: https://learn.microsoft.com/en-us/azure/azure-functions/durable/quickstart-python-vscode?tabs=windows. Use the examples there to locally implement and test it locally and in Azure. Read https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-sequence?tabs=python to learn about various durable fn patterns (particularly function chaining and fan-in/fan-out). You can test the code if you want.\n",
    "- Now, use the examples to implement a simple MapReduce framework. You should have **(i) MasterOrchestrator function** which will call **(ii) Mapper activity functions** first to do mapping and then **(ii) shuffler activity function** that does shuffling, and **(iii) Reducer activity functions** to do reduce, and a **(iv) HTTP client trigger function** that invokes the orchestrator.\n",
    "    - The mapper should take as input a <key, value> pair, where key is the line number and value is the line string, and produce as output a list of key—value pairs, where key is word and value is 1. The mapper tokenize the line into words and reduce <word, 1> pairs.\n",
    "    - The reducer should take as input a key-value pair, where key is a word and value is a list (which will be all 1s). The reducer should add up all the values in the list and produce as output a key—value pair where key is word and value is total count. This is what we saw in lecture 4 – look at the slide deck for pseudocode.\n",
    "    - The shuffler should take map outputs (i.e a list of k-v pairs) and produce the input to reduce fn (<key, [list of values]>).\n",
    "    - TIP: you can implement the orchestrator so that it yields on each call to mapper or reducer for your first implementation. This will simplify things. But later, you should have it run mappers and reducers in parallel. So you should check the fan-out/fan-in pattern to see how you can do parallel task invocations with deferred wait.\n",
    "    - Test the above implementation locally. Feed in fake lines to Mapper from MasterOrchestrator and see if everything works.\n",
    "- Now, we have given you a few input files (mrinput-[1,2,3,4] in moodle) that contain paragraphs from the Saving Mr. Banks song played in class. Create a storage account and upload these files on Azure blobstore in a container. You can do this via the portal and you can find info about blobstore here: https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal\n",
    "- Now, read how to use the python client library to get blobs from the Azure blobstore using their connection string (you can use passwordless if you want, but connection string is easier): https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?tabs=connection-string%2Croles-azure-portal%2Csign-in-azure-cli\n",
    "- Implement a new activity function **GetInputDataFn** that uses the blob store API and connection string to pull down the files from Azure. It should then read each file, break it into lines, and build the overall input to mapreduce which you faked earlier – a list of [<offset, line string>, …] key-value pairs. You can see code for reading from blob store here : https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?tabs=managed-identity%2Croles-azure-portal%2Csign-in-azure-cli.\n",
    "- Modify MasterOrchestrator to call the GetInputDataFn first to get all data. If you have implemented Map and Reduce correctly, everything else should just work like magic! Notice the power of pure functions and clean abstractions.\n",
    "- Deploy the entire solution to azure and test it to make sure that it works. See how many functions\n",
    "got invoked in parallel and marvel at the beauty of Azure Durable functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400e219-8168-4f6e-acd5-c18e7aac20ce",
   "metadata": {},
   "source": [
    "**Deliverable: at the end of this assignment, you have to fill and submit on moodle a file called “Azure-Lab2-Deliverable.docx”, which is available in the assignment 2 section on moodle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf21e9-3e3a-43f8-ae86-67b55b3558a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
