{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd7a58d-dace-4a9d-ae75-e6a12adcb3ef",
   "metadata": {},
   "source": [
    "## what is reinforcement learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40414caa-0204-4143-a090-e34c22fb8e74",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment to achieve a goal. Unlike supervised learning, where the model learns from labeled data, reinforcement learning involves the agent taking actions and receiving feedback in the form of rewards or penalties based on the effectiveness of those actions. Over time, the agent learns a policy (a set of actions) that maximizes the cumulative reward.\n",
    "\n",
    "Here's a breakdown of the key components in reinforcement learning:\n",
    "\n",
    "1. **Agent**: The learner or decision-maker. It interacts with the environment and takes actions based on a policy it develops.\n",
    "\n",
    "2. **Environment**: The external system the agent interacts with. It provides feedback to the agent based on the actions taken.\n",
    "\n",
    "3. **State (S)**: A representation of the current situation the agent is in, typically given by the environment.\n",
    "\n",
    "4. **Action (A)**: A set of possible moves or decisions the agent can make.\n",
    "\n",
    "5. **Reward (R)**: The feedback signal from the environment, which can be positive or negative, indicating the immediate benefit of an action.\n",
    "\n",
    "6. **Policy (π)**: A strategy the agent uses to determine its actions based on the current state.\n",
    "\n",
    "7. **Value Function (V)**: A function estimating the future rewards that can be achieved from a given state, helping the agent plan for long-term success rather than focusing only on immediate rewards.\n",
    "\n",
    "In essence, reinforcement learning aims to create agents that optimize a sequence of actions to achieve the highest possible reward over time. RL is particularly useful for tasks where explicit programming is difficult, such as robotics, game playing (like AlphaGo), and autonomous vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694c5fb-69a0-4933-bc82-b5277ac4e8f1",
   "metadata": {},
   "source": [
    "## who is popular in this discipline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8691abd-0f43-4201-857c-54f973fa36a8",
   "metadata": {},
   "source": [
    "Reinforcement learning has several key figures who have contributed significantly to advancing the field. Here are some of the most influential people in RL:\n",
    "\n",
    "1. **Richard Sutton**: Often considered the \"father of reinforcement learning,\" Sutton co-authored the foundational book *Reinforcement Learning: An Introduction*. His work on temporal difference learning, policy gradient methods, and the development of fundamental RL algorithms has had a lasting impact on the field.\n",
    "\n",
    "2. **Andrew Barto**: A frequent collaborator with Sutton, Barto co-authored *Reinforcement Learning: An Introduction*. His research has also shaped foundational concepts in RL, particularly in the development of algorithms and methods for continuous learning.\n",
    "\n",
    "3. **David Silver**: Known for his work at DeepMind, Silver is one of the lead researchers behind AlphaGo, AlphaZero, and MuZero, which are groundbreaking projects in using RL for complex games like Go, Chess, and Atari games. His contributions to deep reinforcement learning have brought RL to mainstream attention and application.\n",
    "\n",
    "4. **Peter Dayan**: Known for contributions to theoretical neuroscience and its intersection with RL, Dayan has explored how the brain might use similar mechanisms to reinforcement learning. He co-authored *Theoretical Neuroscience* and has worked on the connection between RL and dopamine-driven learning in the brain.\n",
    "\n",
    "5. **John Schulman**: A researcher at OpenAI, Schulman is known for developing advanced RL algorithms like Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). His contributions have significantly advanced deep RL and are commonly used in practical RL applications today.\n",
    "\n",
    "6. **Shimon Whiteson**: Known for his work on multi-agent reinforcement learning and algorithmic improvements, Whiteson's research has focused on ways for multiple agents to learn and collaborate, as well as improving RL for real-world applications.\n",
    "\n",
    "7. **Sergey Levine**: An expert in robotics and reinforcement learning, Levine has done extensive work on using RL for robotic control and real-world applications. His research focuses on ways to bridge the gap between simulation-based RL and real-world environments.\n",
    "\n",
    "8. **Demis Hassabis**: As co-founder and CEO of DeepMind, Hassabis has driven much of the high-profile research in reinforcement learning, such as AlphaGo and AlphaFold. While he’s not solely focused on RL, his leadership has played a crucial role in the practical applications and visibility of RL research.\n",
    "\n",
    "These researchers and leaders have each contributed to specific areas of reinforcement learning, from theoretical foundations to applications in real-world domains. Their work has made RL more accessible and powerful, helping to bridge academic research with practical, impactful applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810faa4f-7642-456f-ac26-3f154abf405d",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e7b70-8e50-4932-a245-414ed48cb6ac",
   "metadata": {},
   "source": [
    "## what are the keywords in RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e2e43-9e52-4927-935b-85091cd7fc06",
   "metadata": {},
   "source": [
    "In reinforcement learning, several core concepts and keywords capture the foundational ideas and methods in the field. Here are some of the most important ones:\n",
    "\n",
    "1. **Agent**: The entity that makes decisions and takes actions in an environment to maximize cumulative reward.\n",
    "\n",
    "2. **Environment**: The external system the agent interacts with, which provides feedback in response to the agent’s actions.\n",
    "\n",
    "3. **State (S)**: The current situation or observation of the environment that the agent can use to make decisions.\n",
    "\n",
    "4. **Action (A)**: A choice the agent makes, which leads to a transition to a new state and a reward.\n",
    "\n",
    "5. **Reward (R)**: The immediate feedback given to the agent after taking an action, used to reinforce desirable behaviors.\n",
    "\n",
    "6. **Policy (π)**: A strategy or mapping from states to actions that defines the agent’s behavior. Policies can be deterministic (always take a specific action in a state) or stochastic (probabilistic actions).\n",
    "\n",
    "7. **Value Function (V)**: A function that estimates the expected return (cumulative reward) from a given state, helping the agent evaluate the potential benefit of being in that state.\n",
    "\n",
    "8. **Action-Value Function (Q)**: Similar to the value function, it estimates the expected return from taking a specific action in a given state. Often called the Q-function.\n",
    "\n",
    "9. **Exploration vs. Exploitation**: The trade-off between exploring new actions to learn more about the environment and exploiting known actions to maximize reward.\n",
    "\n",
    "10. **Markov Decision Process (MDP)**: A mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the control of the agent. MDPs define states, actions, transition probabilities, and rewards.\n",
    "\n",
    "11. **Discount Factor (γ)**: A factor between 0 and 1 that determines the importance of future rewards. A high discount factor makes the agent value future rewards, while a low one makes it focus more on immediate rewards.\n",
    "\n",
    "12. **Temporal Difference (TD) Learning**: A method that combines ideas from Monte Carlo methods and dynamic programming, allowing the agent to learn value estimates directly from raw experience without needing a model of the environment.\n",
    "\n",
    "13. **Q-Learning**: A popular off-policy algorithm that learns the action-value function, allowing agents to learn optimal actions without requiring a model of the environment.\n",
    "\n",
    "14. **SARSA**: An on-policy RL algorithm that updates the Q-value based on the action the agent actually takes in the policy, as opposed to exploring all possible actions.\n",
    "\n",
    "15. **Deep Q-Network (DQN)**: An extension of Q-learning that uses a neural network to approximate the Q-function, making it applicable to high-dimensional state spaces like images.\n",
    "\n",
    "16. **Policy Gradient**: A family of algorithms that optimize the policy directly, often using gradient descent to adjust the parameters of a neural network representing the policy.\n",
    "\n",
    "17. **Actor-Critic**: A method that combines policy-based and value-based methods by having an \"actor\" that decides actions and a \"critic\" that evaluates the actions, providing a balance between learning policies and value functions.\n",
    "\n",
    "18. **Proximal Policy Optimization (PPO)**: A popular policy gradient algorithm that restricts large policy updates to ensure stable learning. It’s commonly used for continuous action spaces and real-world applications.\n",
    "\n",
    "19. **Experience Replay**: A technique where the agent stores past experiences in a replay buffer and samples from it during training. This helps stabilize learning by breaking correlations between consecutive experiences.\n",
    "\n",
    "20. **Multi-Armed Bandit**: A simpler problem in RL where there is only one state and multiple actions, commonly used to study the exploration-exploitation trade-off.\n",
    "\n",
    "21. **Reward Shaping**: Modifying the reward signal to help the agent learn more efficiently by providing additional feedback or incentives.\n",
    "\n",
    "22. **Transfer Learning**: Applying knowledge gained in one RL environment to another, to reduce training time and improve performance in new but related environments.\n",
    "\n",
    "23. **Multi-Agent RL (MARL)**: An extension of RL where multiple agents learn and interact within a shared environment, leading to more complex dynamics and strategies.\n",
    "\n",
    "24. **Self-Play**: A technique where an agent learns by competing or collaborating with itself, commonly used in games to improve performance without needing human opponents.\n",
    "\n",
    "These keywords cover the core concepts, techniques, and algorithms in reinforcement learning, each playing a specific role in how agents learn from interactions with their environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc921e6a-f03f-4a8e-849b-1286f5f4c7b2",
   "metadata": {},
   "source": [
    "In the expression $ J_k(x_k) = \\inf $, the term **$\\inf$** stands for the **infimum**.\n",
    "\n",
    "### What is the Infimum?\n",
    "\n",
    "The **infimum** of a set is the greatest value that is less than or equal to all elements of the set. In other words, it’s the **lowest possible bound** for the values within the set.\n",
    "\n",
    "In the context of optimization and dynamic programming, **infimum** is often used when referring to the smallest cost or value that can be achieved, especially when a minimum might not actually exist within the set (for example, if the set is open). If the minimum does exist, the infimum is the same as the minimum. Otherwise, it represents the lowest bound.\n",
    "\n",
    "### Applying Infimum to the Expression $ J_k(x_k) = \\inf $\n",
    "\n",
    "In dynamic programming:\n",
    "\n",
    "- **$ J_k(x_k) $** typically represents the **cost-to-go** or **value function** at stage $k$ for a particular state $ x_k $.\n",
    "- **$ \\inf $** indicates that we are looking for the smallest possible value of $ J_k(x_k) $ over a set of possible future decisions or controls.\n",
    "\n",
    "So, **$ J_k(x_k) = \\inf $** is shorthand notation meaning that $ J_k(x_k) $ is defined as the lowest possible (infimum) cost-to-go from that state. This infimum is often taken over a set of possible control actions or decisions, aiming to achieve an optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c678815-8593-4aa3-8195-5b7ee57b68e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
