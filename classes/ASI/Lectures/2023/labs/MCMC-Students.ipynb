{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Inference -- MCMC for Bayesian Logistic Regression\n",
    "\n",
    "\n",
    "In this notebook, you will learn how to \n",
    "\n",
    "- Implement the MH algorithm,\n",
    "- Use it to compute classification probabilities.\n",
    "- Understand how to run diagnostics on MCMC runs\n",
    "\n",
    "# 1. Model and data\n",
    "\n",
    "In this lab, you’re going to implement the Metropolis-Hasting algorithm described in the lecture for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import scipy.spatial\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "matplotlib.rc_file('~/.config/matplotlib/matplotlibrc')\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def set_seed(seed: int=0):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Load and plot the data using `np.loadtxt()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, fmt0='oC0', fmt1='oC1', ax=None):\n",
    "    mask = y[:, 0]==1\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot(X[mask, 0], X[mask, 1], fmt0, ms=7, mew=0, alpha=0.7, zorder=0)\n",
    "    ax.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], fmt1, ms=7, mew=0, alpha=0.7, zorder=0)\n",
    "    return ax\n",
    "\n",
    "data = ## *** TO COMPLETE *** ##\n",
    "X = data[...,:-1]\n",
    "y = data[...,-1].reshape(-1,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 4])\n",
    "plot_data(X, y, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression (binary), we use the logistic (or sigmoid) function defined like $h(z) = (1+\\exp(-z))^{-1}$.\n",
    "\n",
    "**Exercise:**\n",
    "Implement the logistic function and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return ## *** TO COMPLETE *** ##\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "fig, ax = plt.subplots(figsize=[4, 3])\n",
    "ax.plot ## *** TO COMPLETE *** ##\n",
    "ax.set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood that we will use is the *Bernoulli likelihood*. Its logdensity is defined as follows\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{y}|\\mathbf{p}) = \\mathbf{y} \\log(\\mathbf{p}) + (1 - \\mathbf{y}) \\log(1 - \\mathbf{p})\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{y}$ is the target class [0, 1] and $\\mathbf{p}$ are the predictive probabilities (i.e. the output of the logistic regression).\n",
    "\n",
    "**Exercise:**\n",
    "Complete the following class to compute the Bernoulli loglikelihood.\n",
    "\n",
    "**Exercise:**\n",
    "Complete also the code for computing the Gaussian logdensity with zero mean and diagonal covariance (check the previous labs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliLikelihood():\n",
    "    def logdensity(self, y, p):\n",
    "        return ## *** TO COMPLETE *** ##\n",
    "\n",
    "class NormalPrior():\n",
    "    def __init__(self, sigma2x):\n",
    "        self.sigma2x = sigma2x\n",
    "        \n",
    "    def logdensity(self, x):\n",
    "        return ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Metropolis-Hastings\n",
    "\n",
    "Check the lecture notes for the full analysis of the sampler. Below you will find just a summary:\n",
    "\n",
    "1. Produces a sequence of samples – $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_s, \\dots$\n",
    "- Imagine we’ve just produced $\\mathbf{w}_{s-1}$\n",
    "- MH firsts proposes a possible $\\mathbf{w}_s$ (call it $\\mathbf{\\tilde w}_s$) based on $\\mathbf{w}_{s-1}$.\n",
    "- MH then decides whether or not to accept wfs\n",
    "    - If accepted, $\\mathbf{w}_s \\leftarrow \\mathbf{\\tilde w}_s$\n",
    "    - If not, $\\mathbf{w}_s \\leftarrow \\mathbf{w}_{s-1}$\n",
    "\n",
    "We need to treat $\\mathbf{\\tilde w}_s$ as a random variable conditioned on $\\mathbf{w}_{s-1}$. We can choose whatever we like but a simple solution is to use a Gaussian centered on  $\\mathbf{w}_{s-1}$ with some covariance $\\mathbf{\\Sigma}_p$. \n",
    "\n",
    "Regarding the acceptance, we need to compute the acceptance ratio. Check the lecture notes for the full derivation.\n",
    "The first thing that we need to compute is the un-normalized logposterior (i.e. the sum of loglikelihood and prior):\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{w}|\\mathbf{X},\\mathbf{y}) \\propto \\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) + \\log p(\\mathbf{w}) := g(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})\n",
    "\\end{align}\n",
    "\n",
    "**Exercise:**\n",
    "Complete the class below with the code to compute the unnormalized logdensity $g(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})$.\n",
    "\n",
    "**Exercise:**\n",
    "Now you can move to the actual MH step. Complete the `step()` function following the flowchart in the slides. Use `self._samples` as buffer for all your samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSampler():\n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples\n",
    "    @samples.getter    \n",
    "    def samples(self):\n",
    "        return np.asarray(self._samples)\n",
    "    \n",
    "    def __init__(self, initial_sample, likelihood, prior):\n",
    "        self.likelihood = likelihood\n",
    "        self.prior = prior\n",
    "        self._samples = [initial_sample]\n",
    "        \n",
    "        \n",
    "    def unnormalized_logposterior(self, w, X, y):\n",
    "        log_likelihood = ## *** TO COMPLETE *** ##\n",
    "        log_prior = ## *** TO COMPLETE *** ##\n",
    "        return log_likelihood + log_prior\n",
    "\n",
    "    def step(self, X, y, step_proposal):\n",
    "        w_prev = self._samples[-1]\n",
    "        w_proposal = ## *** TO COMPLETE *** ##\n",
    "        \n",
    "        \n",
    "        log_gw_prev = ## *** TO COMPLETE *** ##\n",
    "        log_gw_proposal = ## *** TO COMPLETE *** ##\n",
    "        acceptance_ratio = ## *** TO COMPLETE *** ##\n",
    "        \n",
    "        if ## *** TO COMPLETE *** ##\n",
    "            self._samples.append(w_proposal)\n",
    "        else:\n",
    "            u = ## *** TO COMPLETE *** ##\n",
    "            if ## *** TO COMPLETE *** ##\n",
    "                self._samples.append(w_proposal)\n",
    "            else:\n",
    "                self._samples.append(w_prev)\n",
    "        \n",
    "        return min(acceptance_ratio, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Create the likelihood, the prior (with unit variance) and the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "likelihood = ## *** TO COMPLETE *** ##\n",
    "prior = ## *** TO COMPLETE *** ##\n",
    "\n",
    "starting_point = ## *** TO COMPLETE *** ##\n",
    "sampler = ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Run the sampler for 10000 steps (you can fix the step size for the proposal to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "for _ in range(10000):\n",
    "    ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Plot the samples and their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[5, 4])\n",
    "ax.plot ## *** TO COMPLETE *** ##\n",
    "ax.set_xlabel(r'$\\mathbf{w}_0$')\n",
    "ax.set_ylabel(r'$\\mathbf{w}_1$')\n",
    "ax.set_title('Samples from $p(\\mathbf{w}|\\mathbf{X},\\mathbf{y}) $')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make predictions. \n",
    "Remember that our motivation for being Bayesian was to be able to average predictions at $\\mathbf{x}_\\mathrm{new}$, for all possible $\\mathbf{w}$.\n",
    "This is possible by computing the following expectation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n})}h(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}) = \\int h(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}) p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}) \\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "**Exercise:**\n",
    "Complete the next function to compute this expectation. And compute the probability $P (y_\\mathrm{new} = 1 | \\mathbf{x}_\\mathrm{new}, \\mathbf{X}, \\mathbf{y})$ when $\\mathbf{x}_\\mathrm{new} = [2,-4]^\\top$ . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_new, w_samples):\n",
    "    return ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(np.array([2,-4]), sampler.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Now predict on a grid of points and plot the predictive probabilities (use the two helper function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid():\n",
    "    x_grid = np.linspace(-7, 7, 100)\n",
    "    xx, yy = np.meshgrid(x_grid, x_grid)\n",
    "    X_plot = np.vstack((xx.flatten(),yy.flatten())).T\n",
    "    return xx, yy, X_plot\n",
    "\n",
    "def plot_decision_boundary(xx, yy, P, ax=None):   \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    levels = [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1]\n",
    "    cs = ax.contour(xx, yy, P.reshape(*xx.shape), levels,  colors='k', linewidths=1.8, zorder=100);\n",
    "    ax.clabel(cs, inline=1, fontsize=10)\n",
    "    cs = ax.contourf(xx, yy, P.reshape(*xx.shape), levels, cmap='Purples_r', linewidths=0, zorder=0, alpha=0.5);\n",
    "    \n",
    "\n",
    "xx, yy, X_grid = ## *** TO COMPLETE *** ##\n",
    "ps = predict## *** TO COMPLETE *** ##\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 4])\n",
    "plot_decision_boundary(xx, yy, ps, ax=ax)\n",
    "plot_data(X, y, ax=ax )\n",
    "\n",
    "ax.set_xlabel(r'$\\mathbf{x}_0$')\n",
    "ax.set_ylabel(r'$\\mathbf{x}_1$')\n",
    "ax.grid(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Assessing convergence of MCMC\n",
    "\n",
    "Now that the MCMC sampler is done and working, we can move to some analysis.\n",
    "If your algorithm is designed properly, the Markov chain will converge to the target distribution after *infinite* iterations.\n",
    "We need to decide when is it wise to make inferences based on a finite Markov chain.\n",
    "Assessing the convergence of your MCMC is essential if you want to:\n",
    "\n",
    "- Base your conclusions on posterior distributions\n",
    "- Report accurate parameter estimates & uncertainties\n",
    "- Avoid fooling yourself\n",
    "- Avoid devoting resources to follow-up an “inference” that isn’t supported by data\n",
    "- Avoid writing an erratum to your homework\n",
    "\n",
    "## 3.1 Burn-in\n",
    "\n",
    "**Exercise:**\n",
    "Sometimes choosing the initial point to start the MCMC is not easy. Choose some very wrong poins (say $[-4, -4]^\\top$) and run the sampler for 10000 (set the step size to 0.1).\n",
    "Plot the trajectory of the first 200s steps with a different color. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "likelihood = BernoulliLikelihood()\n",
    "prior = NormalPrior(1.)\n",
    "\n",
    "initial_point = ## *** TO COMPLETE *** ##\n",
    "\n",
    "sampler = MHSampler(initial_point, likelihood, prior)\n",
    "\n",
    "for it in range(10000):\n",
    "    ## *** TO COMPLETE *** ##\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 4])\n",
    "ax.plot ## *** TO COMPLETE *** ##\n",
    "ax.plot ## *** TO COMPLETE *** ##\n",
    "ax.set_xlabel(r'$\\mathbf{w}_0$')\n",
    "ax.set_ylabel(r'$\\mathbf{w}_1$')\n",
    "ax.set_title('Samples from $p(\\mathbf{w}|\\mathbf{X},\\mathbf{y}) $')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effect can be mitigated by using burn-in.\n",
    "Burn-in is intended to give the Markov Chain time to reach its equilibrium distribution, particularly if it has started from a lousy starting point. To \"burn in\" a chain, you just discard the first $n$ samples before you start collecting points.\n",
    "\n",
    "The idea is that a \"bad\" starting point may over-sample regions that are actually very low probability under the equilibrium distribution before it settles into the equilibrium distribution. If you throw those points away, then the points which should be unlikely will be suitably rare.\n",
    "\n",
    "It clear that the burn-in is more of a hack/artform than a principled technique. In theory, you could just sample for a really long time or find some way to choose a decent starting point instead. \n",
    "\n",
    "## 3.2 Trace plots\n",
    "\n",
    "The trace plot shows the sampled values of a parameter over time (iterations). This plot helps you to judge how quickly the MCMC procedure converges in distribution—that is, how quickly it forgets its starting values.\n",
    "\n",
    "**Exercise:**\n",
    "For the two parameters of $\\mathbf{w}$, plot their trace. Try to decrease the step parameter, start from 0.5 and go down to 0.01. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "initial_point = np.random.randn(2, 1)\n",
    "\n",
    "fig, axs = plt.subplots(6, 1, figsize=[10, 12], sharex=True)\n",
    "\n",
    "sampler = ## *** TO COMPLETE *** ##\n",
    "## *** TO COMPLETE *** ##\n",
    "axs[0].plot(sampler.samples[..., 0, 0], '.-', ms=3, lw=.2, color='xkcd:windows blue')\n",
    "axs[1].plot(sampler.samples[..., 1, 0], '.-', ms=3, lw=.2, color='xkcd:windows blue')\n",
    "axs[0].set_ylabel(r'$\\mathbf{w}_0$')\n",
    "axs[1].set_ylabel(r'$\\mathbf{w}_1$')\n",
    "axs[0].set_title('Step 0.5')\n",
    "\n",
    "sampler = ## *** TO COMPLETE *** ##\n",
    "## *** TO COMPLETE *** ##\n",
    "axs[0+2].plot(sampler.samples[..., 0, 0], '.-', ms=3, lw=.2, color='xkcd:amber')\n",
    "axs[1+2].plot(sampler.samples[..., 1, 0], '.-', ms=3, lw=.2, color='xkcd:amber')\n",
    "axs[0+2].set_ylabel(r'$\\mathbf{w}_0$')\n",
    "axs[1+2].set_ylabel(r'$\\mathbf{w}_1$')\n",
    "axs[2].set_title('Step 0.1')\n",
    "\n",
    "sampler = ## *** TO COMPLETE *** ##\n",
    "## *** TO COMPLETE *** ##\n",
    "axs[0+4].plot(sampler.samples[..., 0, 0], '.-', ms=3, lw=.2, color='xkcd:dusty purple')\n",
    "axs[1+4].plot(sampler.samples[..., 1, 0], '.-', ms=3, lw=.2, color='xkcd:dusty purple')\n",
    "axs[0+4].set_ylabel(r'$\\mathbf{w}_0$')\n",
    "axs[1+4].set_ylabel(r'$\\mathbf{w}_1$')\n",
    "axs[4].set_title('Step 0.01')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 A more sophisticated diagnostics: $\\hat{R}$ - statistics\n",
    "\n",
    "In an attempt to assuage concerns of poor convergence, we typically run multiple independent chains to see if the\n",
    "obtained distribution is similar across chains. We can also visually inspect the sample paths of the chains via trace plots\n",
    "as well as study summary statistics such as the empirical autocorrelation function.\n",
    "\n",
    "**Exercise:** \n",
    "Complete the following function to run multiple chain (you can use `multiprocessing` for an actual parallel implementation, but it's not required here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chains(num_chains, steps, step_size=.5):\n",
    "    def run_single_chain(steps):\n",
    "        initial_point = ## *** TO COMPLETE *** ##\n",
    "        sampler = ## *** TO COMPLETE *** ##\n",
    "        ## *** TO COMPLETE *** ##\n",
    "        return sampler.samples[...,0]\n",
    "    \n",
    "    samples = ## *** TO COMPLETE *** ##\n",
    "    return np.stack(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Sample now from multiple independent chains and plot the traces (use a step size of 0.5, you can try different configurations if you have time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "samples = ## *** TO COMPLETE *** ##\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=[10, 4], sharex=True)\n",
    "axs[0].plot(samples[..., 0].T, '.-', ms=3, lw=.2,)\n",
    "axs[1].plot(samples[..., 1].T, '.-', ms=3, lw=.2,)\n",
    "axs[0].set_ylabel(r'$\\mathbf{w}_0$')\n",
    "axs[1].set_ylabel(r'$\\mathbf{w}_1$')\n",
    "axs[0].set_title('Trace plot with multiple chains')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the various convergence diagnostics, probably the most widely used is the **potential scale reduction factor** $\\hat R$.\n",
    "It is recommended as the primary convergence diagnostic in widely applied software\n",
    "packages for MCMC sampling such as Stan (Carpenter et al., 2017), JAGS (Plummer, 2003), WinBUGS (Lunn et al.,\n",
    "2000), OpenBUGS (Lunn et al., 2009), PyMC3 (Salvatier et al., 2016), and NIMBLE (de Valpine et al., 2017), which\n",
    "together are estimated to have hundreds of thousand of users. \n",
    "$\\hat R$ is computed for each scalar quantity of interest, as the standard deviation of that quantity from all the chains included together, divided by the root mean square of the separate within-chain standard deviations. The idea is that if a set of simulations have not mixed well, the variance\n",
    "of all the chains mixed together should be higher than the variance of individual chains\n",
    "\n",
    "\n",
    "At convergence, the chains will have mixed, so that the distribution of the simulations\n",
    "between and within chains will be identical, and the ratio $\\hat R$ should equal 1. If $\\hat R$\n",
    "is greater than 1, this implies that the chains have not fully mixed and that further simulation might increase the precision of inferences. In practice we typically go until $\\hat R$ is less than 1.1/1.05 for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rhat_base(ary):\n",
    "    \"\"\"Compute the rhat for a 2d array.\"\"\"\n",
    "    _, num_samples = ary.shape\n",
    "\n",
    "    # Calculate chain mean\n",
    "    chain_mean = np.mean(ary, axis=1)\n",
    "    # Calculate chain variance\n",
    "    chain_var = np.var(ary, axis=1, ddof=1)\n",
    "    # Calculate between-chain variance\n",
    "    between_chain_variance = num_samples * np.var(chain_mean, axis=None, ddof=1)\n",
    "    # Calculate within-chain variance\n",
    "    within_chain_variance = np.mean(chain_var)\n",
    "    # Estimate of marginal posterior variance\n",
    "    rhat_value = np.sqrt(\n",
    "        (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n",
    "    )\n",
    "    return rhat_value\n",
    "\n",
    "\n",
    "def _rhat_rank(ary):\n",
    "    \"\"\"Compute the rank normalized rhat. \n",
    "    Computation follows https://arxiv.org/abs/1903.08008\n",
    "    \"\"\"\n",
    "    \n",
    "    def _z_scale(ary):\n",
    "        rank = scipy.stats.rankdata(ary, method=\"average\")\n",
    "        z = scipy.stats.norm.ppf((rank - 0.5) / ary.size)\n",
    "        return z.reshape(ary.shape)\n",
    "    \n",
    "    \n",
    "    def _split_chains(ary):\n",
    "        \"\"\"Split and stack chains.\"\"\"\n",
    "        _, n_draw = ary.shape\n",
    "        half = n_draw // 2\n",
    "        return np.vstack((ary[:, :half], ary[:, -half:]))\n",
    "    \n",
    "    split_ary = _split_chains(ary)\n",
    "    rhat_bulk = _rhat_base(_z_scale(split_ary))\n",
    "\n",
    "    split_ary_folded = abs(split_ary - np.median(split_ary))\n",
    "    rhat_tail = _rhat_base(_z_scale(split_ary_folded))\n",
    "\n",
    "    rhat_rank = max(rhat_bulk, rhat_tail)\n",
    "    return rhat_rank\n",
    "\n",
    "def compute_rhat(samples):\n",
    "    \"\"\"Compute the rhat statistics from samples. Samples needs to be a tensor \n",
    "    with dimensions [num_of_chain, num_of_samples, num_of_variables]. \"\"\"\n",
    "    \n",
    "    samples = np.atleast_3d(samples)\n",
    "    return np.asarray([_rhat_rank(samples[...,i]) for i in range(samples.shape[-1]) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Using the function provided above, compute the $\\hat R$-statistics for $\\mathbf{w}$ with the following parameters:\n",
    "\n",
    "- num_of_chain = 4\n",
    "- num_of_samples = [100, 1000, 10000]\n",
    "- step size = [0.5, 0.1, 0.01]\n",
    "\n",
    "**Question:**\n",
    "Comment the results. For which configurations $\\hat R$-statistics suggests convergence has not been achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "The $\\hat R$-statistics can also be plotted as a function of iteration. Complete the next code cell to visualize its behaviour.\n",
    "Try also with step size = [0.5, 0.1, 0.01]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "samples = ## *** TO COMPLETE *** ##\n",
    "\n",
    "steps = np.logspace(1, 4, 10).astype(int)\n",
    "rhats = ## *** TO COMPLETE *** ##\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[8,3])\n",
    "ax.plot(steps, ## *** TO COMPLETE *** ##\n",
    "\n",
    "ax.fill_between(steps, 0.9, 1.05, color='xkcd:leaf green', alpha=.5, lw=0)\n",
    "ax.fill_between(steps, 1.05, 1.15, color='xkcd:pumpkin orange', alpha=.5, lw=0)\n",
    "ax.set_title(r'$\\hat R$ vs iterations')\n",
    "ax.set_ylim(0.9, 2)\n",
    "ax.legend([r'$\\mathbf{w}_0$', r'$\\mathbf{w}_1$'])\n",
    "ax.semilogx()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. A real example: MAGIC dataset (+)\n",
    "\n",
    "The data is made of 10 features and it's generated to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique.\n",
    "\n",
    "The objective is to discriminate those caused by primary gammas celestial objects (signal) from the ones generated by cosmic rays scattered in the upper atmosphere (background). \n",
    "\n",
    "**Exercise:**\n",
    "Try to solve this problem. Divide the dataset into train/test set (0.8/0.2). Set up the prior, the likelihood and the sampler. Sample and make sure you reached convergence. Predict the test set and report some metrics (test accuracy and test loglikelihood are the most commonly used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "data = np.loadtxt('magic.csv', delimiter=',')\n",
    "data_size = len(data)\n",
    "data = data[np.random.permutation(data_size)]\n",
    "\n",
    "X = data[...,:-1]\n",
    "y = data[...,-1].reshape(-1, 1)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "initial_point = ## *** TO COMPLETE *** ##\n",
    "prior = ## *** TO COMPLETE *** ##\n",
    "likelihood = ## *** TO COMPLETE *** ##\n",
    "sampler = ## *** TO COMPLETE *** ##\n",
    "\n",
    "## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ## *** TO COMPLETE *** ##\n",
    "\n",
    "test_loglikelihood = ## *** TO COMPLETE *** ##\n",
    "test_accuracy = ## *** TO COMPLETE *** ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
