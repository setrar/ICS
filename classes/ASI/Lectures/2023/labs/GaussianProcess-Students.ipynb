{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Inference -- Gaussian Process for Regression\n",
    "\n",
    "Today, we will play with Gaussian processes. By the end of this lab, you will be able to \n",
    " \n",
    "- To sample from a Gaussian process prior distribution.\n",
    "- To implement Gaussian process inference for regression.\n",
    "- To use the above to observe samples from a Gaussian process posterior distribution.\n",
    "- To evaluate how different hyperparameter settings impact model quality.\n",
    "- To investigate different kernel functions and parameter optimisation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian processes (henceforth GPs) achieve greater flexibility over parametric models by imposing a preference bias as opposed to restrictive constraints. Although the parameterisation of GPs allows one to access a certain (infinite) set of functions, preference can be expressed using a prior over functions. This allows greater freedom in representing data dependencies, thus enabling the construction of better-suited models. In this lab, we shall cover the basic concepts of GP regression. For the sake of clarity, we shall focus on univariate data, which allows for better visualisation of the GP model. Nonetheless, the code implemented within this lab can be very easily extended to handle\n",
    "multi-dimensional inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import scipy.spatial\n",
    "\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc_file('~/.config/matplotlib/matplotlibrc')\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def set_seed(seed: int=0):\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sampling from the GP Prior\n",
    "\n",
    "We shall consider a one-dimensional regression problem, whereby the inputs x are transformed by\n",
    "a function $f(x) = 2sin(exp(0.03 * x))$. \n",
    "\n",
    "**Exercise:**\n",
    "Generate 200 random points, $x$, in the range $[-20, 80]$, and compute their corresponding function\n",
    "values, $\\mathbf{f}$. The target function can then be plotted accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "def f(x):\n",
    "    return ## *** TO COMPLETE *** ##\n",
    "\n",
    "X = ## *** TO COMPLETE *** ##\n",
    "y = ## *** TO COMPLETE *** ##\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5,3])\n",
    "ax.plot(X, y, '.k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that since GPs are non-parametric; we define a prior distribution over functions (models),\n",
    "specified as a multivariate Gaussian distribution $p(f) = \\mathcal{N} (\\mu, \\Sigma)$.\n",
    "\n",
    "Without loss of generality, we shall assume a zero-mean GP prior, i.e. $\\mu = 0$. The covariance\n",
    "matrix of the distribution, $\\Sigma$, may then be computed by evaluating the covariance between the\n",
    "input points. For this tutorial, we shall consider the widely used squared-exponential (RBF)\n",
    "covariance (also referred to as the kernel function), which is defined between two points as: \n",
    "\n",
    "$$k(x, x') = \\sigma_\\mathrm{f}^2 \\exp \\Big( -\\dfrac {(x-x')^2}{2l^2} \\Big). $$\n",
    "\n",
    "This kernel is parameterised by a lengthscale parameter $l$, and variance $\\sigma_\\mathrm{f}^2$ . Given that the true\n",
    "function may be assumed to be corrupted with noise, we can also add a noise parameter, $\\sigma_\\mathrm{n}^2$ , to\n",
    "the diagonal entries of the resulting kernel matrix, $K$, such that\n",
    "\n",
    "$$K_y = K + \\sigma_\\mathrm{n}^2I.$$\n",
    "\n",
    "\n",
    "**Exercise:**\n",
    "Complete the `compute_kernel()` function for computing the RBF kernel $K$ between two sets of input points.\n",
    "Hint: The `cdist` function in scipy can be used for evaluating the pairwise Euclidean distance between two sets of points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel:\n",
    "    def compute_squared_distance(self, X, Z):\n",
    "        return ## *** TO COMPLETE *** ##\n",
    "    \n",
    "    def __call__(self, X, Z=None):\n",
    "        Z = X if Z is None else Z  # Just a simple trick\n",
    "        return self.compute_kernel(X, Z)\n",
    "    \n",
    "\n",
    "class RBF(Kernel):\n",
    "    def __init__(self, lengthscale, variance):\n",
    "        self.lengthscale = lengthscale\n",
    "        self.variance = variance\n",
    "    \n",
    "    def compute_kernel(self, X, Z):\n",
    "        sq_dists = ## *** TO COMPLETE *** ##\n",
    "        return ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the kernel matrix $K(\\mathbf{X}, \\mathbf{X})$. Plot it using `plt.matshow()`. Start with $\\sigma^2_\\mathrm{f}$ and $l$ both equal to one. Then try to change this two parameter. What do you see? Comment the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ## *** TO COMPLETE *** ##\n",
    "Kxx = ## *** TO COMPLETE *** ##\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow ## *** TO COMPLETE *** ##\n",
    "ax.grid(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to sample from the GP prior. Remember if $\\mathbf{f}$ follows a GP, ie $\\mathbf{f} \\sim \\mathcal{GP}(\\mu, \\mathbf{\\Sigma})$, then $p(\\mathbf{f}) = \\mathcal{N}(\\mu, \\mathbf{\\Sigma})$.\n",
    "\n",
    "**Exercise:**\n",
    "Assuming a zero-mean prior, and using the RBF kernel constructed before, we can sample from the prior distribution using the numpy `multivariate_normal()` function.\n",
    "For the time being, you can initialise the kernel parameters as follows:\n",
    "\n",
    "- lengthscale = 10\n",
    "- variance = 0.1\n",
    "\n",
    "You can use the following helper function to plot the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(x, mean, cov, palette=\"Greens\", fill_alpha=0.95, ax=None, fill_kwargs={},):\n",
    "    \"\"\" A helper function for plotting 1D GP from mean and cov \"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    cmap = plt.get_cmap(palette)\n",
    "    ci = np.linspace(0.5, 3, 50)\n",
    "\n",
    "    colors = (ci - np.min(ci)) / (np.max(ci) - np.min(ci) + 3)+.1\n",
    "    \n",
    "    x = x.flatten()\n",
    "\n",
    "    ax.plot(x, mean, color=cmap(0.9))\n",
    "    for i, c in enumerate(ci[::-1]):\n",
    "        upper = mean + c * np.diag(cov)\n",
    "        lower = mean - c * np.diag(cov)\n",
    "        color_val = colors[i]\n",
    "        ax.fill_between(x, upper, lower, color=cmap(color_val), alpha=fill_alpha, **fill_kwargs)\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_gp_from_samples(x, samples, palette=\"Greens\",ax=None, alpha=0.75, **kwargs):\n",
    "    \"\"\" A helper function for plotting 1D GP samples \"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    cmap = plt.get_cmap(palette)\n",
    "    ci = np.linspace(0.5, 5, 50)\n",
    "\n",
    "    colors = (ci - np.min(ci)) / (np.max(ci) - np.min(ci) + 3)#+.1\n",
    "    samples = samples.T\n",
    "        \n",
    "    idx = np.random.randint(0, samples.shape[1], 20)\n",
    "    ax.plot(x, samples[:,idx], color=cmap(0.5), lw=1, alpha=alpha, **kwargs)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "kernel = RBF(10, .1)\n",
    "\n",
    "mean = ## *** TO COMPLETE *** ##\n",
    "cov = ## *** TO COMPLETE *** ##\n",
    "samples = ## *** TO COMPLETE *** ##\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "\n",
    "plot_gp_from_samples(X, samples, ax=axs[0])\n",
    "plot_gp(X, mean, cov, ax=axs[1])\n",
    "\n",
    "axs[1].plot(X, y, '.k', label='Data points')\n",
    "axs[0].set_title(r'Samples from the $\\mathcal{GP}$ prior')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try to change the kernel hyperparameters and plot few samples from their corresponding $\\mathcal{GP}$ prior. What do you observe?\n",
    "\n",
    "# 2. Inference with GP\n",
    "\n",
    "\n",
    "Recall that the prior represents our prior beliefs before observing the function values of any data points. Suppose we can now observe 3 points at random from the input data; we would expect that with this additional knowledge, the functions drawn from the updated GP distribution would be constrained to pass through these points (or at least close if corrupted with noise). The combination of the prior and the likelihood of the observed data leads to the posterior distribution over functions.\n",
    "\n",
    "We will wrap everything in a class, thus to make everything easier to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Assign 3 points at random from $\\mathbf{X}$ (and their corresponding function values) to `Xtrain` and `ytrain`\n",
    "respectively. For now we shall assume that all other $\\mathbf{X}$ values are unobserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "idx = ## *** TO COMPLETE *** ##\n",
    "Xtrain = X[idx]\n",
    "ytrain = y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell has a skeleton for the GP model. Onward on the notebook, you will be required to go back here and implement the different functions now blank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPR():\n",
    "    \"Class to implement a Gaussian process model for regression\"\n",
    "    def __init__(self, X, y, kern):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.kern = kern\n",
    "        self.likelihood_variance = 1e-3\n",
    "    \n",
    "    def sample_f_prior(self, Xtest, nsamples=1):\n",
    "        raise NotImplementedError('Implement this function to sample from the GP prior.') # Remove this line when done\n",
    "            \n",
    "    def predict_f_posterior(self, Xtest):\n",
    "        raise NotImplementedError('Implement this function to predict mean and var of the GP posterior') # Remove this line when done\n",
    "    \n",
    "    def predict_log_likelihood(self):\n",
    "        raise NotImplementedError('Implement this function to compute the log likelihood') # Remove this line when done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Complete the function `sample_f_prior` in the class above. Create a new GP model and verify you get the same results as above when you use `sample_f_prior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ## *** TO COMPLETE *** ##\n",
    "model = ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lecture, you have derived the posterior distribution of the GP given data $\\{\\mathbf{X},\\mathbf{y}\\}$ (check the lecture note if you don't remeber).\n",
    "\n",
    "**Note**: As we have encountered in previous labs, matrix inversions can be both numerically troublesome and slow to compute. In this lab, we shall avoid computing matrix inversions directly by instead considering Cholesky decompositions for solving linear systems. You are encouraged to read more about Cholesky decompositions for GPs by consulting Appendix A.4 of [Gaussian Processes for Machine Learning (Rasmussen and Williams, 2005)](http://www.gaussianprocess.org/gpml/) - available online!\n",
    "The complete pseudo-code for the posterior inference procedure is provided in Algorithm 2.1 from Chapter 2 of this same book.\n",
    "Unfortunately, that Algorithm explains how to compute the posterior one test point at a time. You could loop through all test points but this is not very efficient. Instead you can compute the posterior for all points in one shot, by remembering that\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{f_\\star}\\,|\\,\\mathbf{y}) &= \\mathcal{N}(\\mathbf{f}_{\\text{mean}}, \\mathbf{f}_{\\text{cov}}) \\\\\n",
    "\\mathbf{f}_{\\text{mean}} &=  \\kappa(\\mathbf{X}_\\star, \\mathbf{X})\\left[\\kappa(\\mathbf{X}, \\mathbf{X}) + \\sigma_\\mathrm{n}^2 I\\right]^{-1}\\mathbf{y}\\\\\n",
    "\\mathbf{f}_{\\text{cov}} &=  \\kappa(\\mathbf{X}_\\star, \\mathbf{X}_star) - \\kappa(\\mathbf{X}_\\star, \\mathbf{X})\\left[\\kappa(\\mathbf{X}, \\mathbf{X}) + \\sigma_\\mathrm{n}^2 I\\right]^{-1}\\kappa(\\mathbf{X}_\\star, \\mathbf{X})^\\top\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "**Exercise:** \n",
    "In the class GP, complete the function to compute the posterior GP (`predict_f_posterior`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Run the previously completed function to compute the GP posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean, f_cov = ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Sample few times from the posterior and, using the helper functions defined above, plot the samples and their distribution. Note that we should also add the noise variance to the predictive covariance of the posterior. Fix this accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "samples = ## *** TO COMPLETE *** ##\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=[10, 3], sharey=True)\n",
    "\n",
    "plot_gp_from_samples ## *** TO COMPLETE *** ##\n",
    "plot_gp ## *** TO COMPLETE *** ##\n",
    "\n",
    "axs[0].plot(Xtrain, ytrain, 'Xk',)\n",
    "axs[1].plot(Xtrain, ytrain, 'Xk', label='Data points')\n",
    "axs[0].set_title(r'Samples from the $\\mathcal{GP}$ post.')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "As a measure of model quality, you should also compute the log marginal likelihood of the model.\n",
    "To this end, complete the code provided above to include the negative log likelihood term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to add more samples, let's say 5, 10 and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to change the kernel parameters (lengthscale and variance). Plot the posterior for 3/4 combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Run a grid search over a range of parameter values in order to determine which configuration yields the best result (based on the marginal).\n",
    "Pick 20 samples and run a grid search with $\\sigma_\\mathrm{f}\\in[0.5, 5]$ and $l\\in[6, 12]$. *Hint:* To speed up this evaluation, note that you don't need to compute the predictive posterior, you just need the marginal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "idx = ## *** TO COMPLETE *** ##\n",
    "Xtrain = X[idx]\n",
    "ytrain = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenghtscales = np.linspace(6, 12, 100)\n",
    "variances = np.linspace(.5, 5, 100)\n",
    "marginal_ll = []\n",
    "\n",
    "for l in lenghtscales:\n",
    "    for v in variances:\n",
    "        ## *** TO COMPLETE *** ##\n",
    "\n",
    "marginal_ll = np.array(marginal_ll).reshape(len(lenghtscales), len(variances))\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.gca()\n",
    "cset = ax.contourf(variances, lenghtscales, marginal_ll, cmap='coolwarm',)\n",
    "fig.colorbar(cset)\n",
    "cset = ax.contour(variances, lenghtscales, marginal_ll, colors='k',)\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('Variance')\n",
    "ax.set_ylabel('Lenghtscale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (+) Alternative Kernel Functions\n",
    "So far we have focused exclusively on the RBF kernel.\n",
    "However, the choice of kernel function (along with its associated parameters) can have a significant effect on the overall Gaussian process model.\n",
    "Choosing the best kernel to fit your data is no simple task, and is a pertinent problem in many applied domains.<br>\n",
    "\n",
    "A brief discussion on this problem may be found here: <a target=\"_blank\" href=\"https://www.cs.toronto.edu/~duvenaud/cookbook/\">Kernel Cookbook</a>. \n",
    "\n",
    "Familiarise yourself better with these issues by implementing one or two additional kernels. \n",
    "\n",
    "For example, another popular kernel used in GP literature is the **Matérn kernel**.\n",
    "The Matérn covariance between two points separated by $d=||\\mathbf{x}-\\mathbf{z}||_2^2$ distance units is given by [Rasmussen & Williams](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf) (Ch. 4).\n",
    "\n",
    "\\begin{align}\n",
    "C_\\nu(d) = \\sigma^2\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\Bigg(\\sqrt{2\\nu}\\frac{d}{\\rho}\\Bigg)^\\nu K_\\nu\\Bigg(\\sqrt{2\\nu}\\frac{d}{\\rho}\\Bigg),\n",
    "\\end{align}\n",
    "\n",
    "where $\\Gamma$ is the Gamma function, $K_\\nu$ is the modified Bessel function of the second kind, and $\\rho$ and $\\nu$ are non-negative parameters of the covariance.\n",
    "\n",
    "In practice:\n",
    "\n",
    "- for $\\nu = 1/2$:    $\\qquad C_{1/2}(d) = \\sigma^2\\exp\\left(-\\frac{d}{\\rho}\\right)$,\n",
    "\n",
    "- for $\\nu = 3/2$:    $\\qquad C_{3/2}(d) = \\sigma^2\\left(1+\\frac{\\sqrt{3}d}{\\rho}\\right)\\exp\\left(-\\frac{\\sqrt{3}d}{\\rho}\\right)$,\n",
    "\n",
    "- for $\\nu = 5/2$:    $\\qquad C_{5/2}(d) = \\sigma^2\\left(1+\\frac{\\sqrt{5}d}{\\rho}+\\frac{5d^2}{3\\rho^2}\\right)\\exp\\left(-\\frac{\\sqrt{5}d}{\\rho}\\right)$.\n",
    "\n",
    "**Exercise:**\n",
    "Pick one/two versions of the Matérn kernel and implement them. Plot the prior from the GP with this new kernel. Run the inference task and report/plot the results. *Hint:* You can reuse the code for the RBF kernel to compute the distances and you don't need to change the inference function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matern32(Kernel):\n",
    "    def __init__(self, rho, variance):\n",
    "        ## *** TO COMPLETE *** ##\n",
    "        \n",
    "    def compute_kernel(self, X, Z):\n",
    "        ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parameter Optimisation using Gradient Descent\n",
    "\n",
    "Optimise the hyperparameters of the model by minimising the negative log-likelihood of the model. For a complete solution, you should include the derivatives of the objective function with respect to the parameters being optimised.\n",
    "\n",
    "The general formula for computing the derivative is given below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\theta_i} = - \\frac{1}{2} \\textbf{Tr} \\left( K^{-1} \\frac{\\partial K}{\\partial \\theta_i} \\right) + \\frac{1}{2} \\mathbf{y}^{T} K^{-1} \\frac{\\partial K}{\\partial \\theta_i} K^{-1} \\mathbf{y}.\n",
    "\\end{equation}\n",
    "\n",
    "To give a more concrete example, the $\\frac{\\partial K}{\\partial \\theta_i}$ term for the lengthscale parameter in the RBF kernel is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial K}{\\partial l} = \\sigma_\\mathrm{f}^2 \\exp \\left( -\\dfrac {(\\mathbf{x}-\\mathbf{x}')^2}{2l^2} \\right)\\left( \\dfrac {(\\mathbf{x}-\\mathbf{x}')^2}{l^3} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Then, you can use them to run vanilla Gradient descent. For example, \n",
    "\n",
    "\\begin{align}\n",
    "l^{(t+1)} &= l^{(t)} + \\eta \\frac{\\partial\\mathcal{L}}{\\partial l^{(t)}} \\\\\n",
    "&= l^{(t)}  + \\eta \\left[\\frac{1}{2} \\textbf{Tr} \\left( K^{-1} \\frac{\\partial K}{\\partial l} \\right) + \\frac{1}{2} \\mathbf{y}^{T} K^{-1} \\frac{\\partial K}{\\partial l} K^{-1} \\mathbf{y}\\right].\n",
    "\\end{align}\n",
    "\n",
    "## The \"Millenial\" version\n",
    "\n",
    "We don't really need to derive the gradients of the marginal likelihood w.r.t. parameters by hand. We can leverage the automatic differentiation engine in PyTorch! All the operations that we need are all differentiable. Let's take advantage of that! (But you are free to go for the classic style -- if you want and/or you are not familiar with PyTorch).\n",
    "\n",
    "\n",
    "**Pro tip:** Note that the parameters $l$, $\\sigma_\\mathrm{f}^2$ , and $\\sigma_\\mathrm{n}^2$ are always expected to be positive. It is possible that the optimisation algorithm attempts to evaluate the log-likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. A commonly-used technique to enforce this condition is to work with a transformed version of covariance parameters using the logarithm transformation. In particular, define $\\psi_l = \\log(l)$, $\\psi_\\mathrm{f} = \\log(\\sigma_\\mathrm{f}^2 )$, and $\\psi_\\mathrm{n} = \\log(\\sigma_\\mathrm{n}^2 )$, and optimise with respect to the $\\Psi$ parameters. The optimisation problem in the transformed space is now unbounded, and the gradient of the log-likelihood should be computed with respect to the $\\Psi$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "def args_as_tensors(*index):\n",
    "    def decorator(method):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            converted_args = [torch.tensor(a).float() \n",
    "                              if i in index and type(a) is np.ndarray else a \n",
    "                              for i,a in enumerate(args)]\n",
    "            return method(*converted_args, **kwargs)\n",
    "        return wrapper  \n",
    "    return decorator\n",
    "\n",
    "class RBF(nn.Module):\n",
    "    @args_as_tensors(0, 1)\n",
    "    def compute_squared_distance(X, Z):\n",
    "        return torch.cdist(X, Z) ** 2\n",
    "    \n",
    "    def __init__(self, lengthscale, variance):\n",
    "        super(RBF, self).__init__()\n",
    "        self.loglengthscale = nn.Parameter(torch.tensor(float(lengthscale)).log())\n",
    "        self.logvariance = nn.Parameter(torch.tensor(float(variance)).log())\n",
    "        \n",
    "    def __call__(self, X, Z=None):\n",
    "        if Z is None: \n",
    "            Z = X\n",
    "        return torch.exp(- 0.5 * torch.cdist(X, Z) ** 2 / (self.loglengthscale.exp() ** 2)) * self.logvariance.exp() \n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return \"lengthscale = %.2f\\nvariance = %.2f\" % (self.loglengthscale.exp(), self.logvariance.exp())\n",
    "\n",
    "\n",
    "class GPR(nn.Module):\n",
    "    @args_as_tensors(2, 3)\n",
    "    def __init__(self, X, y, kernel, noisevar=1e-3):\n",
    "        super(GPR, self).__init__() \n",
    "        self.kern = kernel\n",
    "        self.N = len(X)\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y)\n",
    "        self.logvariance = nn.Parameter(torch.tensor(noisevar).log())\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return \"noise_variance = %1.2e\" % self.logvariance.exp()\n",
    "    \n",
    "    def compute_likelihood(self,):        \n",
    "        K = self.kern(self.X) + torch.eye(self.X.shape[0]) * self.logvariance.exp()\n",
    "        L = torch.cholesky(K, upper=False)\n",
    "        V, _ = torch.solve(self.y.reshape(-1, 1), L)\n",
    "        ll = - 0.5 * (self.X.shape[0] * (np.log(2 * np.pi)) + torch.log(torch.diag(L)).sum() + (V**2).sum())\n",
    "        return ll\n",
    "   \n",
    "    @args_as_tensors(1)    \n",
    "    def predict_f_posterior(self, Xtest):\n",
    "        Kx = self.kern(self.X, Xtest)\n",
    "        K = self.kern(self.X) + torch.eye(self.X.shape[0]) * self.logvariance.exp()\n",
    "        L = torch.cholesky(K, upper=False)\n",
    "        A = torch.solve(Kx, L)  \n",
    "        V, _ = torch.solve(self.y.reshape(-1, 1), L)\n",
    "        fmean = A.T @ V\n",
    "        fvar = self.kern(Xtest) - A.T @ A\n",
    "        return fmean, fvar\n",
    "\n",
    "    \n",
    "set_seed()\n",
    "idx = np.random.permutation(len(X))[:20]\n",
    "Xtrain = X[idx]\n",
    "ytrain = y[idx]\n",
    "        \n",
    "kernel = RBF(1, 1) \n",
    "model = GPR(Xtrain, ytrain, kernel)\n",
    "print(model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Most of the stuff is done, so you don't need to worry too much. Check carefully the code above and make sure you are happy with all operations.\n",
    "Write now the main optimization loop following the sketch below (NB. All pytorch optimizers expect to minimize an objective function, not maximize -- fix this accordingly). Also, save the loss at every step and plot it.\n",
    "\n",
    "```\n",
    "for n iterations:\n",
    "    reset the gradients\n",
    "    compute the loss\n",
    "    backpropagate\n",
    "    update the parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ll = []\n",
    "for _ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = ## *** TO COMPLETE *** ##\n",
    "    ## *** TO COMPLETE *** ##\n",
    "    h_ll.append(loss.detach().numpy())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[7, 3])\n",
    "ax.plot ## *** TO COMPLETE *** ##\n",
    "ax.margins(0, 0.05)\n",
    "ax.set_title('Optimization of the kernel parameters')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Finally, with these parameters, plot the predictive posterior as we did before (complete the function `predict()` above)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
