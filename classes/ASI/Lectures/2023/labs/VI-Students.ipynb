{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Inference -- Variational Inference for Bayesian Logistic Regression\n",
    "\n",
    "In this notebook, you will learn how to implement the (stochastic) variational inference algorithm.\n",
    "\n",
    "**NOTE:** In this notebook we will heavily rely on [PyTorch](https://pytorch.org/). Syntax is pretty similar to numpy's for what we need to do -- don't worry.\n",
    "The main different is the programming style, much more object oriented. If you are not familiar with OOP, the Internet is a [good friend](https://lmgtfy.com/?q=oop+python). \n",
    "\n",
    "This gist will serve you as a refresh on Variational Inference (VI).\n",
    "In the general setting, given a probabilistic model with observations $\\{\\mathbf{X},\\mathbf{y}\\}$, model parameters $\\mathbf{w}$ and likelihood $p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w})$, by introducing an approximate posterior distribution $q_\\theta(\\mathbf{w})$ with parameters $\\theta$, the variational lower bound to the log-marginal likelihood is defined as\n",
    "\n",
    "\n",
    "\\begin{align} \n",
    "  \\mathrm{KL}[{q_\\theta(\\mathbf{w})}||{p(\\mathbf{w}|\\mathbf{X},\\mathbf{y})}] & = \n",
    "      \\mathbb{E}_{q_{\\theta}} \\left[ \\log\\frac{q_\\theta(\\mathbf{w})}{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y})} \\right] =  \\nonumber \\\\\n",
    "  & =   \\mathbb{E}_{q_{\\theta}}\\left[\\log q_{\\theta}(\\mathbf{w}) - \\log p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y})\\right] = \\nonumber \\\\\n",
    "  & =   \\mathbb{E}_{q_{\\theta}}\\left[-\\log p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w})\\right] + \\mathbb{E}_{q_{\\theta}}\\left[\\log q_{\\theta}(\\mathbf{w}) - \\log p(\\mathbf{w}) \\right] + \\log p(\\mathbf{y}|\\mathbf{X}) = \\nonumber \\\\\n",
    "  & =  -\\mathbb{E}_{q_{\\theta}}\\log p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}) +  \\mathrm{KL}[{q_{\\theta}(\\mathbf{w})}||{p(\\mathbf{w})}] + \\log p(\\mathbf{y}|\\mathbf{X}) \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The previous derivation shows that minimizing the KL divergence between the approximate posterior and the true posterior is equivalent to minimizing the so-called Negative Expected Lower Bound (NELBO).\n",
    "This also shows that when the approximate posterior is exactly equal to the true posterior, the NELBO is equal to the negative log-marginal likelihood.  \n",
    "\n",
    "The objective is then to minimize this variational bound:\n",
    "\n",
    "\\begin{align}\n",
    "      \\mathcal{L}(\\theta) = -\\underbrace{\\mathbb{E}_{q_{\\theta}}\\log p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w})}_\\text{Expected loglikelihood} +  \\mathrm{KL}[{q_{\\theta}(\\mathbf{w})}||{p(\\mathbf{w})}]\n",
    "\\end{align}\n",
    "\n",
    "The analytic evaluation of the NELBO is generally still untractable due to the presence of the expected loglikelihood under the variational distribution (in the majority of the cases the rightmost KL is tractable).\n",
    "This is commonly overcome by sampling $N_\\mathrm{MC}$ times from $q_\\theta$ \n",
    "\\begin{align}\n",
    "    \\label{eq:expected-loglikelihood}\n",
    "    \\mathbb{E}_{q_{\\theta}}\\log p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}) \\approx \\dfrac{1}{N_\\mathrm{MC}} \\sum_{\\tilde{\\mathbf{w}}_i\\sim q_\\theta} \\log p(\\mathbf{y}|\\mathbf{X}, \\tilde{\\mathbf{w}}_i)\n",
    "\\end{align}\n",
    "\n",
    "In case of large datasets, even this formulation can be computationally challenging, due to the evaluation of the likelihood $N_\\mathrm{MC}$ times.\n",
    "Nonetheless, assuming fully factorization on the likelihood, this quantity can be approximated even further using mini-batching.\n",
    "Let $\\mathcal{D}$ be the dataset made of $N$ examples, taking a random subset $\\mathcal{B} \\subset \\mathcal{D}$, the approximation becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{q_{\\theta}}\\log p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w} \\approx \\dfrac{1}{N_\\mathrm{MC}} \\frac{N}{|\\mathcal{B}|}\\sum_{\\tilde{\\mathbf{w}}_i\\sim q_\\theta} \\sum_{\\mathbf{X}_j, \\mathbf{y}_j\\sim\\mathcal{B}} \\log p(\\mathbf{y}_j|\\mathbf{X}_j, \\tilde{\\mathbf{w}}_i)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "# 1. Setup and data\n",
    "\n",
    "Similarly to the previous lab, youâ€™re going to implement the VI algorithm described in the lecture for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import scipy.spatial\n",
    "import time \n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "matplotlib.rc_file('~/.config/matplotlib/matplotlibrc')\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def set_seed(seed: int=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "def args_as_tensors(*index):\n",
    "    \"\"\"A simple decorator to convert numpy arrays to torch tensors\"\"\"\n",
    "    def decorator(method):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            converted_args = [torch.tensor(a).float() \n",
    "                              if i in index and type(a) is np.ndarray else a \n",
    "                              for i, a in enumerate(args)]\n",
    "            return method(*converted_args, **kwargs)\n",
    "        return wrapper  \n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining some probability distributions that we will need in this notebook. \n",
    "First, the Bernoulli distribution. \n",
    "\n",
    "**Exercise:**\n",
    "Complete the following class to compute the logdensity of the Bernoulli distribution \n",
    "\n",
    "(*Hint:* check the previous lab for the solution) (*Hint 2:* to avoid NaN while computing logs, add a small positive constant, e.g. jitter, to the logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = 1e-10\n",
    "\n",
    "class Distribution(nn.Module):  \n",
    "    pass\n",
    "        \n",
    "class Bernoulli(Distribution):\n",
    "    @args_as_tensors(1, 2)\n",
    "    def logdensity(self, y, p):\n",
    "        return ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Now let's move to the Gaussian distribution. Similarly for what we did on the GP lab, note that the parameter $\\sigma^2$ is always expected to be positive. It is possible that the optimisation algorithm attempts to evaluate the log-likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. A commonly-used technique to enforce this condition is to work with a transformed version of parameters using the logarithm transformation. In particular, define $\\psi = \\log\\sigma^2$. So remember to take the exponential of $\\psi$ if you want to use $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDiagonal(Distribution):\n",
    "    @property\n",
    "    def var(self):\n",
    "        return self.logvar.exp()\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'train=%s' % self.train\n",
    "    \n",
    "    def __init__(self, d, train=True):\n",
    "        super(NormalDiagonal, self).__init__()\n",
    "        self.train = train\n",
    "        self.d = d\n",
    "        self.mean = nn.Parameter(torch.zeros(d), requires_grad=train)\n",
    "        self.logvar = nn.Parameter(torch.zeros(d), requires_grad=train)\n",
    "    \n",
    "    def sample(self, n=1):\n",
    "        eps = ## *** TO COMPLETE *** ##\n",
    "        samples = ## *** TO COMPLETE *** ##    # shape: [n x self.d]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. KL Divergence\n",
    "\n",
    "Run the cell below -- there are a couple of helper functions that will make the notebook lighter for coding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering\n",
    "\n",
    "_KL_REGISTRY = {}  # Source of truth mapping a few general (type, type) pairs to functions.\n",
    "_KL_MEMOIZE = {}  # Memoized version mapping many specific (type, type) pairs to functions.\n",
    "\n",
    "@total_ordering\n",
    "class _Match(object):\n",
    "    __slots__ = ['types']\n",
    "\n",
    "    def __init__(self, *types):\n",
    "        self.types = types\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.types == other.types\n",
    "\n",
    "    def __le__(self, other):\n",
    "        for x, y in zip(self.types, other.types):\n",
    "            if not issubclass(x, y):\n",
    "                return False\n",
    "            if x is not y:\n",
    "                break\n",
    "        return True\n",
    "\n",
    "def _dispatch_kl(type_q, type_p):\n",
    "    matches = [(super_q, super_p) for super_q, super_p in _KL_REGISTRY\n",
    "               if issubclass(type_q, super_q) and issubclass(type_p, super_p)]\n",
    "    if not matches:\n",
    "        return NotImplemented\n",
    "    left_q, left_p = min(_Match(*m) for m in matches).types\n",
    "    right_p, right_q = min(_Match(*reversed(m)) for m in matches).types\n",
    "    left_fun = _KL_REGISTRY[left_q, left_p]\n",
    "    right_fun = _KL_REGISTRY[right_q, right_p]\n",
    "    if left_fun is not right_fun:\n",
    "        logger.warning('Ambiguous kl_divergence({}, {}). Please register_kl({}, {})'.format(\n",
    "            type_q.__name__, type_p.__name__, left_q.__name__, right_p.__name__))\n",
    "    return left_fun\n",
    "\n",
    "\n",
    "def register_kl(type_q, type_p):\n",
    "    \"\"\"\n",
    "    Decorator to register a pairwise function with kl_divergence.\n",
    "    Usage:\n",
    "\n",
    "        @register_kl(Normal, Normal)\n",
    "        def kl_normal_normal(q, p):\n",
    "            # insert implementation here\n",
    "    \"\"\"\n",
    "    if not isinstance(type_q, type) and issubclass(type_q, BaseDistribution):\n",
    "        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n",
    "    if not isinstance(type_p, type) and issubclass(type_p, BaseDistribution):\n",
    "        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n",
    "    \n",
    "    def decorator(fun):\n",
    "        _KL_REGISTRY[type_q, type_p] = fun\n",
    "        _KL_MEMOIZE.clear()  # reset since lookup order may have changed\n",
    "        print('KL divergence between \\'%s\\' and \\'%s\\' registered.' % (type_q.__name__, type_p.__name__))\n",
    "        return fun\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def kl_divergence(q, p):\n",
    "    r\"\"\"Compute Kullback-Leibler divergence KL(p|q) between two distributions.\"\"\"\n",
    "    try:\n",
    "        fun = _KL_MEMOIZE[type(q), type(p)]\n",
    "    except KeyError:\n",
    "        fun = _dispatch_kl(type(q), type(p))\n",
    "        _KL_MEMOIZE[type(q), type(p)] = fun\n",
    "    if fun is NotImplemented:\n",
    "        raise NotImplementedError('KL divergence for pair %s - %s not registered' % (type(q).__name__,\n",
    "                                                                                     type(p).__name__))\n",
    "    return fun(q, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression of the KL divergence between multivariate Gaussians $q = \\mathcal{N}(\\mathbf{\\mu}_q, \\Sigma_q)$ and $p = \\mathcal{N}(\\mathbf{\\mu}_p, \\Sigma_p)$ is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{KL}[q || p] = \n",
    "\\frac{1}{2} \\mathrm{Tr}(\\Sigma_p^{-1} \\Sigma_q)\n",
    "+ \\frac{1}{2} (\\mu_p - \\mu_q)^{\\top} \\Sigma_1^{-1} (\\mu_p - \\mu_q)\n",
    "- \\frac{D}{2} \n",
    "+ \\frac{1}{2} \\log\\left( \\frac{\\mathrm{det}\\Sigma_p}{\\mathrm{det}\\Sigma_q} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This formula simplifies when the two Gaussians have diagonal covariance, i.e. $q = \\mathcal{N}(\\mathbf{\\mu}_q, \\mathbf{\\sigma}^2_q\\mathrm{I})$ and $p = \\mathcal{N}(\\mathbf{\\mu}_p, \\mathbf{\\sigma}^2_p\\mathrm{I})$,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{KL}[q || p] =  \\frac{1}{2} \\sum\\left( \\log \\frac{\\sigma^2_p}{\\sigma^2_q} + \\frac{\\sigma_q^2 + (\\mu_q - \\mu_p)^2}{\\sigma_p^2} - 1 \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Exercise:** \n",
    "Complete the next function to compute the KL divergence between two multivariate Gaussian distribution with diagonal covariance. *Note:* Since we have parameterized the Gaussian distribution with the logvariance, the formula above can be simplified even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_kl(NormalDiagonal, NormalDiagonal)\n",
    "def _normaldiagonal_normaldiagonal(q, p):\n",
    "    kl = ## *** TO COMPLETE *** ##\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Create two identical Gaussian distributions and compute the KL divergence using the function `kl_divergence(...)`. What's the result? Is it what you were expecting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ## *** TO COMPLETE *** ##\n",
    "q = ## *** TO COMPLETE *** ##\n",
    "\n",
    "kl_divergence(## *** TO COMPLETE *** ##)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model\n",
    "\n",
    "Now we can move to design the model. We will use a simple logistic regression very similarly to what done in the previous MCMC lab.\n",
    "This very simple model computes $h(\\mathbf{x}) = h(\\mathbf{w}^\\top\\mathbf{x})$, where $h(\\cdot)$ is the logistic function. \n",
    "\n",
    "**Exercise:** \n",
    "Complete the next class, by constructing the prior and (approximate) posterior over $\\mathbf{w}$ (*Hint:* use `[input_dim, 1]` for the dimension of $\\mathbf{w}$ ). Complete also the `predict_y` function. Remember that you need to sample `mc_samples` times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return ## *** TO COMPLETE *** ##\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.prior_w = ## *** TO COMPLETE *** ##\n",
    "        self.posterior_w = ## *** TO COMPLETE *** ##\n",
    "        \n",
    "    @args_as_tensors(1)\n",
    "    def predict_y(self, X, mc_samples=1):\n",
    "        w_samples = ## *** TO COMPLETE *** ##\n",
    "        y_samples = ## *** TO COMPLETE *** ##\n",
    "        return y_samples\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variational objective\n",
    "\n",
    "The objective is to minimize this variational bound:\n",
    "\\begin{align}\n",
    "      \\mathcal{L}(\\theta) = -\\underbrace{\\mathbb{E}_{q_{\\theta}}\\log p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w})}_\\text{Expected loglikelihood} +  \\mathrm{KL}[{q_{\\theta}(\\mathbf{w})}||{p(\\mathbf{w})}]\n",
    "\\end{align}\n",
    "\n",
    "**Exercise:**\n",
    "Complete the next cell to compute the NELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalObjective(nn.Module):    \n",
    "    def __init__(self, model, likelihood, N, mc_samples=1):\n",
    "        super(VariationalObjective, self).__init__()\n",
    "        self.N = N\n",
    "        self.model = model\n",
    "        self.likelihood = likelihood\n",
    "        self.mc_samples = mc_samples\n",
    "        \n",
    "    def expected_loglikelihood(self, Xbatch, ybatch):\n",
    "        ypred = ## *** TO COMPLETE *** ##\n",
    "        logliks = ## *** TO COMPLETE *** ##\n",
    "        return ## *** TO COMPLETE *** ##\n",
    "    \n",
    "    def kl(self):\n",
    "        return ## *** TO COMPLETE *** ##\n",
    "    \n",
    "    def compute_objective(self, Xbatch, ybatch):\n",
    "        return ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Load the data from the `binaryclass2` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ## *** TO COMPLETE *** ##\n",
    "X = data[...,:-1]\n",
    "y = data[...,-1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Create the likelihood and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = ## *** TO COMPLETE *** ##\n",
    "model = ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Create the NELBO (use 10 Monte Carlo samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelbo = ## *** TO COMPLETE *** ##\n",
    "print(nelbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Try to compute the variational objective using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Analysis of the MC estimate of the NELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that everything is done and ready we can start to make some analysis.\n",
    "\n",
    "First of all, as we said we don't have an analytical formula for the variational objective (our loss). We can only access (unbiased) samples, hence the next question.\n",
    "\n",
    "**Exercise:**\n",
    "Try to sample the NELBO 20 times with [1, 10, 100, 1000] MC samples and plot their distribution with boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "nelbo_samples = {}\n",
    "num_of_nelbo_samples = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    nelbo.mc_samples = 1\n",
    "    nelbo_samples['1'] = [## *** TO COMPLETE *** ## for _ in range(num_of_nelbo_samples)]\n",
    "\n",
    "    nelbo.mc_samples = 10\n",
    "    nelbo_samples['10'] = ## *** TO COMPLETE *** ##\n",
    "\n",
    "    ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[11, 3])\n",
    "\n",
    "sns.boxplot(data=pd.DataFrame(nelbo_samples), ax=ax,  whis=np.inf)\n",
    "ax.set_title('Samples of the MC estimate of the NELBO')\n",
    "ax.set_xlabel('MC samples')\n",
    "ax.set_ylabel('NELBO')\n",
    "ax.margins(0,0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said that, in case of large datasets, this can be computationally challenging, due to the evaluation of the likelihood $N_\\mathrm{MC}$ times.\n",
    "But we know that the NELBO can be approximated even further using mini-batching.\n",
    "Taking a random subset of data $\\mathcal{B}$, the approximation becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{q_{\\theta}}\\log p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}) \\approx \\dfrac{1}{N_\\mathrm{MC}} \\frac{N}{|\\mathcal{B}|}\\sum_{\\tilde{\\mathbf{w}}_i\\sim q_\\theta} \\sum_{\\mathbf{X}_j, \\mathbf{y}_j\\sim\\mathcal{B}} \\log p(\\mathbf{y}_j|\\mathbf{X}_j, \\tilde{\\mathbf{w}}_i)\n",
    "\\end{align}\n",
    "\n",
    "This introduces even more variance in the estimate of the NELBO but it allows to scale to (virtually) any sized dataset.\n",
    "Run the next cell to create a dataset object with a minibatch size of just 1 sample. You can retrive a batch of data by calling the `next_batch()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, X, y, minibatch_size):\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.minibatch_size = min(minibatch_size, len(self.X))\n",
    "        self._i = 0  \n",
    "    def next_batch(self):  \n",
    "        if len(self.X) <= self._i + self.minibatch_size:\n",
    "            shuffle = np.random.permutation(len(self.X))\n",
    "            self.X = self.X[shuffle]\n",
    "            self.y = self.y[shuffle]\n",
    "            Xbatch = self.X[self._i:]\n",
    "            ybatch = self.y[self._i:]\n",
    "            self._i = 0\n",
    "            return Xbatch, ybatch\n",
    "\n",
    "        Xbatch = self.X[self._i:self._i + self.minibatch_size]\n",
    "        ybatch = self.y[self._i:self._i + self.minibatch_size]\n",
    "        self._i += self.minibatch_size\n",
    "        return Xbatch, ybatch\n",
    "    \n",
    "dataset = Dataset(X, y, minibatch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execise:**\n",
    "Similarly as before, try to sample the NELBO 20 times with [1, 10, 100, 1000] MC samples but now fix the minibatch size to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "nelbo_samples = {}\n",
    "num_of_nelbo_samples = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    nelbo.mc_samples = 1\n",
    "    nelbo_samples['1'] = ## *** TO COMPLETE *** ##\n",
    "\n",
    "    ## *** TO COMPLETE *** ##\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[11, 3])\n",
    "sns.boxplot(data=pd.DataFrame(nelbo_samples), ax=ax,  whis=np.inf )\n",
    "ax.set_title('Samples of the MC estimate of the NELBO with minibatches')\n",
    "ax.set_xlabel('MC samples')\n",
    "ax.set_ylabel('NELBO')\n",
    "ax.margins(0,0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Do you see any difference? Comment the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimization\n",
    "\n",
    "Ok, now we can move to the optimization of the NELBO. From a methodology point of view there's nothing new here: reset the gradient (required by pytorch), get a batch of data, compute the objective, backpropagate and, finally, update the parameters. \n",
    "\n",
    "Let's start with the binary classification dataset as it's easier to understand and plot.\n",
    "\n",
    "**Exercise:**\n",
    "Load the data from the `binaryclass2.csv` (without minibatching), create the likelihood, the model and the NELBO (use 1000 MC samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ## *** TO COMPLETE *** ##\n",
    "X = data[...,:-1]\n",
    "y = data[...,-1].reshape(-1,1)\n",
    "dataset = Dataset(X, y, minibatch_size=1000)\n",
    "\n",
    "likelihood = ## *** TO COMPLETE *** ##\n",
    "model = ## *** TO COMPLETE *** ##\n",
    "\n",
    "nelbo = ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next class is just a simple util that we will use to store some values as optimization evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary:\n",
    "    @property\n",
    "    def data(self):\n",
    "        data = pd.DataFrame(self._data, columns=['step', self.name, 'time'])\n",
    "        data.time = data.time - data.time.iloc[0]\n",
    "        return data\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"A simple class to store some values during optimization\"\"\"\n",
    "        self.name = str(name)\n",
    "        self._data = []\n",
    "    \n",
    "    def append(self, step, value):\n",
    "        self._data.append([step, float(value.detach().numpy()), time.time()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Optimize the NELBO. We suggest to use SGD optimizer the 1e-3 a learning rate (you can increase it -- if you want -- this is a simple problem). At every step, store the value of the nelbo, of the expected likelihood and the KL using the class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelbo_summary = Summary('nelbo')\n",
    "nll_summary = Summary('expected_loglik')\n",
    "kl_summary = Summary('kl')\n",
    "\n",
    "optim = ## *** TO COMPLETE *** ##\n",
    "num_iterations = 1000\n",
    "\n",
    "for step in range(num_iterations):\n",
    "    ## *** TO COMPLETE *** ##\n",
    "    Xbatch, ybatch = ## *** TO COMPLETE *** ##\n",
    "    loss = ## *** TO COMPLETE *** ##\n",
    "    \n",
    "    nelbo_summary.append(step, loss)\n",
    "    nll_summary.append(step, loss - nelbo.kl())\n",
    "    kl_summary.append(step, nelbo.kl())\n",
    "\n",
    "    ## *** TO COMPLETE *** ##\n",
    "    ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Assess convergence of the optimization by plotting the tree metrics (plot the expected_loglik and the KL in a different plot, in logscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[10, 3])\n",
    "\n",
    "nelbo_summary.data.plot(x='step', y='nelbo', ax=axs[0]);\n",
    "nll_summary.data.plot(x='step', y='expected_loglik', ax=axs[1], c='C1');\n",
    "kl_summary.data.plot(x='step', y='kl', ax=axs[1], c='C2');\n",
    "axs[1].semilogy();\n",
    "fig.suptitle('Optimization of the NELBO', y=1.02)\n",
    "axs[0].margins(0, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Analyze the behaviour of these three values and comment the plots. Focus you analysis on the breakdown of the NELBO in its constituents parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Now the predictions! \n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y})}h(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}) = \\int h(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}) p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}) \\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "With 100 samples, compute the probability $P (y_\\mathrm{new} = 1 | \\mathbf{x}_\\mathrm{new}, \\mathbf{X}, \\mathbf{y})$ when $\\mathbf{x}_\\mathrm{new} = [2,-4]^\\top$ (check the method `predict_y` of `model` and use at least 100 samples). Compare the result with the number you got from the previous lab.\n",
    "\n",
    "**NB:** By default, PyTorch always allocates memory and computations for the gradients. For inference, this is not needed -- hence the use of `with torch.no_grad()` (It's a context-manager that disabled gradient calculation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execise:**\n",
    "Let's now plot the distribution at convergence and the predictions on a grid of points. Compare it with the MCMC samples of the previous lab (you can find them in `./mcmc_samples.npy`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_pdf(distr, ax=None):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    x_grid = np.linspace(0, 3, 100)\n",
    "    xx, yy = np.meshgrid(x_grid, x_grid)\n",
    "    Xplot = np.vstack((xx.flatten(),yy.flatten())).T\n",
    "    m = distr.mean.detach().numpy()[...,0]\n",
    "    c = distr.logvar.exp().detach().numpy() * np.eye(2)\n",
    "    rv = scipy.stats.multivariate_normal(mean=m, cov=c)\n",
    "    points = rv.pdf(Xplot)\n",
    "    ax.contour(xx, yy, points.reshape(*xx.shape), colors='k', linewidths=1.5, zorder=100);\n",
    "    ax.set_xlabel(r'$\\mathbf{w}_0$')\n",
    "    ax.set_ylabel(r'$\\mathbf{w}_1$')\n",
    "\n",
    "\n",
    "def plot_data(X, y, fmt0='oC0', fmt1='oC1', ax=None):\n",
    "    mask = y[:, 0]==1\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot(X[mask, 0], X[mask, 1], fmt0, ms=7, mew=0, alpha=0.7, zorder=0)\n",
    "    ax.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], fmt1, ms=7, mew=0, alpha=0.7, zorder=0)\n",
    "    return ax\n",
    "\n",
    "def get_grid():\n",
    "    x_grid = np.linspace(-6, 6, 100)\n",
    "    xx, yy = np.meshgrid(x_grid, x_grid)\n",
    "    Xplot = np.vstack((xx.flatten(),yy.flatten())).T\n",
    "    return xx, yy, Xplot\n",
    "\n",
    "def plot_decision_boundary(xx, yy, P, ax=None):   \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    levels = [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1]\n",
    "    cs = ax.contour(xx, yy, P.reshape(*xx.shape), levels,  colors='k', linewidths=1.8, zorder=100);\n",
    "    ax.clabel(cs, inline=1, fontsize=10)\n",
    "    cs = ax.contourf(xx, yy, P.reshape(*xx.shape), levels, cmap='Purples_r', linewidths=0, zorder=0, alpha=0.5);\n",
    "    ax.set_xlabel(r'$\\mathbf{x}_0$')\n",
    "    ax.set_ylabel(r'$\\mathbf{x}_1$')\n",
    "\n",
    "\n",
    "mcmc_samples = np.load('mcmc_samples.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[8, 4])\n",
    "xx, yy, Xplot = get_grid()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = ## *** TO COMPLETE *** ##\n",
    "\n",
    "plot_decision_boundary(xx, yy, predictions, ax=axs[0])\n",
    "plot_data(X, y, ax=axs[0])\n",
    "plot_gaussian_pdf(model.posterior_w, axs[1])\n",
    "axs[1].plot(*mcmc_samples, '.', c='xkcd:deep orange', ms=5, label='MCMC samples')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "At convergence, sample 1000 times the NELBO like we did before, take its negative (the ELBO) plot it with a boxplot (use `torch.no_grad()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_nelbos = {}\n",
    "with torch.no_grad():\n",
    "    converged_nelbos['Mean Field'] = ## *** TO COMPLETE *** ##\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=[5,3])\n",
    "sns.boxplot(data=pd.DataFrame(converged_nelbos),  whis=np.inf )\n",
    "ax.margins(0.05)\n",
    "ax.set_title('ELBO at convergence', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(+) Exercise:** \n",
    "The powerfull aspect of (stochastic) variational inference is that it can scale very easily to any model and any-sized dataset. \n",
    "If you have time, try to solve the classification problem of the MiniBoo dataset; this dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background).\n",
    "\n",
    "**NB:** With its more than 100000 data points each of them with 50 features, this is definetely a bigger dataset: you need to reduce the number of MC samples and the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Alternatives to Mean Field Variational Inference\n",
    "\n",
    "As we saw from one of the previous question, the approximation that we used is very rough: it recoves some properties of the true posterior but fails to capture the strong correlation that exists in the parameter space. \n",
    "What we need to do it to increase the complexity and the expressiveness of the variational posterior. \n",
    "The first step that we can do is to introduce a non-diagonal covariance $\\mathbf{w} \\sim \\mathcal{N}(\\mu, \\Sigma)$, where the covariance $\\Sigma=LL^\\top$ ($L$ is a lower triangular matrix).\n",
    "\n",
    "Sampling from such distribution is possible again using the reparameterization trick,\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{w}} = \\mu + \\mathrm{Tril}(L)\\mathbf{\\varepsilon} \\quad \\mathbf{\\varepsilon} \\sim \\mathcal{N}(0, \\mathrm{I})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathrm{Tril}(\\cdot)$ returns the lower triangular part of the matrix (the other elements are set to 0).\n",
    "\n",
    "**Exercise:** \n",
    "Complete the next class to model a full covariance Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalFullCovariance(Distribution): \n",
    "    def extra_repr(self):\n",
    "        return 'train=%s' % self.train\n",
    "    \n",
    "    def __init__(self, d, train=True):\n",
    "        super(NormalFullCovariance, self).__init__()\n",
    "        self.train = train\n",
    "        self.d = d\n",
    "        self.mean = nn.Parameter(torch.zeros(d), requires_grad=train)\n",
    "        self.cholesky_cov = nn.Parameter(torch.eye(d), requires_grad=train)\n",
    "    \n",
    "    def sample(self, n=3):\n",
    "        eps = ## *** TO COMPLETE *** ##\n",
    "        samples = ## *** TO COMPLETE *** ##\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compute the KL divergence between $q$ Gaussian with full covariance and $p$ Gaussian with diagonal covariance. \n",
    "Remember that the expression of the KL divergence between multivariate Gaussians $q = \\mathcal{N}(\\mathbf{\\mu}_q, \\Sigma_q)$ and $p = \\mathcal{N}(\\mathbf{\\mu}_p, \\Sigma_p)$ is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{KL}[q || p] = \n",
    "\\frac{1}{2} \\mathrm{Tr}(\\Sigma_p^{-1} \\Sigma_q)\n",
    "+ \\frac{1}{2} (\\mu_p - \\mu_q)^{\\top} \\Sigma_p^{-1} (\\mu_p - \\mu_q)\n",
    "- \\frac{D}{2} \n",
    "+ \\frac{1}{2} \\log\\left( \\frac{\\mathrm{det}\\Sigma_p}{\\mathrm{det}\\Sigma_q} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "**Question:**\n",
    "Given that $q = \\mathcal{N}(\\mathbf{\\mu}_q, LL^\\top)$ and $p = \\mathcal{N}(\\mathbf{\\mu}_p, \\sigma^2_p\\mathrm{I})$, write the simplified KL divergence. \n",
    "\n",
    "*Hints:* \n",
    "\n",
    "- $\\mathrm{Tr}(LL^\\top) = \\sum\\mathrm{diag}(L)^2$ \n",
    "\n",
    "- $\\log\\mathrm{det}(LL^\\top) = \\sum\\log\\mathrm{diag}(L)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you'll find the KL implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_kl(NormalFullCovariance, NormalDiagonal)\n",
    "def _kl_normalfullcov_normaldiagonal(q, p):\n",
    "    kl = (\n",
    "        torch.sum(p.logvar.exp() * torch.diag(q.cholesky_cov)**2) + \n",
    "        torch.sum(p.logvar.exp() * (p.mean - q.mean) ** 2) +\n",
    "        torch.sum(p.logvar) + \n",
    "        torch.sum(torch.log(torch.diag(q.cholesky_cov)) ** 2) -\n",
    "        len(q.mean)\n",
    "    )\n",
    "    return .5 * kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Now we need to modify the logistic regression model by using the new `NormalFullCovariance` as posterior over $\\mathbf{w}$. Complete the next class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.prior_w = ## *** TO COMPLETE *** ##\n",
    "        self.posterior_w = ## *** TO COMPLETE *** ##\n",
    "        \n",
    "    @args_as_tensors(1)\n",
    "    def predict_y(self, X, mc_samples=1):\n",
    "        w_samples = ## *** TO COMPLETE *** ##\n",
    "        y_samples = ## *** TO COMPLETE *** ##\n",
    "        return y_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! If you did everything correctly you don't need to change anything more.\n",
    "\n",
    "**Exercise:**\n",
    "Train this new model exactly the same as before. Plot the progression of the NELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = ## *** TO COMPLETE *** ##\n",
    "model = ## *** TO COMPLETE *** ##\n",
    "\n",
    "nelbo = ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelbo_summary = Summary('nelbo')\n",
    "\n",
    "optim = ## *** TO COMPLETE *** ##\n",
    "num_iterations = 500\n",
    "\n",
    "for step in range(num_iterations):\n",
    "    ## *** TO COMPLETE *** ##\n",
    "    \n",
    "    nelbo_summary.append(step, loss)\n",
    "    \n",
    "    ## *** TO COMPLETE *** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "\n",
    "nelbo_summary.data.plot(x='step', y='nelbo', ax=ax);\n",
    "fig.suptitle('Optimization of the NELBO', y=1.02)\n",
    "ax.margins(0, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Do the same plot as before, by showing predictions and distribution (together with MCMC samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_pdf(distr, ax=None):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    x_grid = np.linspace(0, 3, 100)\n",
    "    xx, yy = np.meshgrid(x_grid, x_grid)\n",
    "    Xplot = np.vstack((xx.flatten(),yy.flatten())).T\n",
    "    m = distr.mean.detach().numpy()\n",
    "    c = distr.cholesky_cov.detach().numpy()\n",
    "    c = c @ c.T\n",
    "    rv = scipy.stats.multivariate_normal(mean=m, cov=c)\n",
    "    points = rv.pdf(Xplot)\n",
    "    ax.contour(xx, yy, points.reshape(*xx.shape), colors='k', linewidths=1.5, zorder=100);\n",
    "    ax.set_xlabel(r'$\\mathbf{w}_0$')\n",
    "    ax.set_ylabel(r'$\\mathbf{w}_1$')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=[8, 4])\n",
    "xx, yy, Xplot = get_grid()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = ## *** TO COMPLETE *** ##\n",
    "\n",
    "## *** TO COMPLETE *** ##\n",
    "axs[1].plot(*mcmc_samples, '.', c='xkcd:deep orange', ms=5, label='MCMC samples')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "As before sample 1000 times the NELBO, take its negative (the ELBO) plot it with a boxplot, together with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    converged_nelbos['Full Cov'] = ## *** TO COMPLETE *** ##\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=[5,3])\n",
    "sns.boxplot(data=pd.DataFrame(converged_nelbos),  whis=np.inf, linewidth=.75)\n",
    "ax.margins(0.05)\n",
    "ax.set_title('ELBO at convergence', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Do you observe something interesting? \n",
    "Remember what the ELBO represents. It is the lower bound of the marginal distribution $p(\\mathbf{y}|\\mathbf{X})$. Check the first lab on Bayesian linear regression if you don't remember what the marginal distribution measures. Based solely on this value, which model would you choose? Why?    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
