{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Inference -- Bayesian Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, you'll start to implement some basic Bayesian models, \n",
    "starting from the simple Bayesian linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import scipy.linalg\n",
    "import scipy.stats\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def set_seed(seed: int=0):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. A regression dataset\n",
    "Create a simple 1D regression dataset using the `make_regression(...)` function and plot it.\n",
    "For the moment, keep the noise variance $\\sigma_\\mathrm{n}$ small.\n",
    "NB. For better reproducibility, please remember to fix the Numpy's random seed. \n",
    "For Jupyter notebooks, this needs to be done at the beginning of all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_regression(n: int, sigma2noise: float = .1):\n",
    "    X = np.random.uniform(-3, 3, n)\n",
    "    y = 5 - .25 * X  + 0.5 * X ** 3 \n",
    "    y += np.sqrt(sigma2noise) * np.random.randn(*X.shape)\n",
    "    return X[:, None], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "sigma2noise = .1\n",
    "X, y = make_regression(20, sigma2noise=sigma2noise)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5,3])\n",
    "ax.plot(X, y, 'o')\n",
    "ax.set_title('A regression dataset')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.margins(0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. A review on Gaussian likelihood\n",
    "Let's start from the basics. Remember what a likelihood is.\n",
    "The **likelihood** measures the goodness of fit of a statistical model to samples of data for given values of \n",
    "the unknown model parameters.\n",
    "It's computed from the joint probability distribution, but viewed and used as **function** \n",
    "of the parameters only, thus treating the random variables as fixed at the observed values.\n",
    "\n",
    "A Gaussian likelihood is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\prod_{i=1}^N p(y_i|\\mathbf{w}, {\\mathbf{x}}_i, \\sigma_\\mathrm{n}) = \\prod_{i=1}^N \\mathcal{N}(y_i|\\tilde y_i, \\sigma_\\mathrm{n})\n",
    "\\end{equation}\n",
    "\n",
    "where, for linear regression, $\\tilde y_i = \\mathbf{w}^\\top {\\mathbf{x}}_i$.\n",
    "For numerical stability, instead of using the vanilla likelihood, we will use the **log-likelihood**.\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) =  \\sum_{i=1}^N  \\log\\mathcal{N}(y_i|\\tilde y_i, \\sigma_\\mathrm{n})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Write a function to compute the log-density of a normal distribution at position $x$, given $\\mu$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lognormal(x, mu, va):\n",
    "    return # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "For the moment, assume that for the sample $i^{\\mathrm{th}}$, you predict $\\tilde y = 0.3$ and $\\sigma_\\mathrm{n} = 1$. \n",
    "You know that $y = 0.4$. \n",
    "Complete the following function `gaussian_loglik(...)`, then compute the (log)likelihood for this sample and show its position on the Gaussian density with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_loglik(y, y_tilde, sigma2noise):\n",
    "    return # TO COMPLETE\n",
    "\n",
    "\n",
    "def plot_gaussian(mu: np.ndarray, var: np.ndarray, plot_log: bool = False, **kwargs):\n",
    "    \"\"\" A simple util to plot a gaussian pdf \"\"\"\n",
    "    x = np.linspace(mu - 5 * np.sqrt(var), mu + 5 * np.sqrt(var), 100)\n",
    "    y = lognormal(x, mu, var) if plot_log else np.exp(lognormal(x, mu, var))\n",
    "    ax = kwargs.pop('ax', plt.gca())\n",
    "    ax.plot(x, y, **kwargs)\n",
    "    return ax\n",
    "\n",
    "\n",
    "y_obs =        # TO COMPLETE\n",
    "y_tilde =      # TO COMPLETE\n",
    "sigma2noise =  # TO COMPLETE\n",
    "\n",
    "sample_logl =  # TO COMPLETE \n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "plot_gaussian(y_tilde, sigma2noise, ax=ax, label=r'$\\mathcal{N}(\\tilde y, \\sigma_n^2)$')\n",
    "ax.vlines(y_obs, 0, np.exp(sample_ll))\n",
    "ax.vlines(y_tilde, 0, np.exp(gaussian_loglik(y_tilde, y_tilde, sigma2noise)), ls='--')\n",
    "ax.plot(y_obs, np.exp(sample_ll), 'ok', label=r'Likelihood')\n",
    "ax.text(y_tilde+0.1, 0.02, r'$\\tilde y$')\n",
    "ax.text(y_obs-0.2, 0.02, r'$y$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bayesian Linear regression\n",
    "\n",
    "In this section, you'll start to implement the Bayesian linear regression model.\n",
    "Let's start by creating the **design matrix** $\\mathbf{X}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[ {\\begin{array}{ccccc}\n",
    "   1 & x_1^1 & \\dots & x_1^K\\\\\n",
    "   1 & x_2^1 & \\dots & x_2^K\\\\\n",
    "   \\vdots &    \\vdots & &   \\vdots \\\\\n",
    "   1 & x_N^1 & \\dots & x_N^K\\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "**Exercise:**\n",
    "Complete the following function `build_X(...)` to build $\\mathbf{X}$.\n",
    "This can be done in many ways. One of them is using a double list comprehension (one index for the row and one for the column), while another one is using the numpy `column_stack()` function (highly suggested). In any case, inspect $\\mathbf{X}$ to make sure it looks OK (show the first entries). To fit higher order polynomials, we need to add extra columns to $\\mathbf{X}$, therefore build it with $K$ as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X(X, K):\n",
    "    assert K > 0 and isinstance(K, int)\n",
    "    \n",
    "    return # TO COMPLETE\n",
    "\n",
    "build_X(X, 2)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture notes, let's define the prior on the parameters $\\mathbf{w}$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{S}) \n",
    "\\end{equation}\n",
    "\n",
    "For sake of simplicity, assume the covariance matrix $\\mathbf{S}$ to be diagonal $\\mathbf{S} = \\sigma_\\mathrm{w}^2\\mathbf{I}$.\n",
    "Remember that the likelihood is defined as $p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{X}\\mathbf{w}, \\sigma_\\mathrm{n}^2\\mathbf{I})$.\n",
    "In this case, the posterior is analitic and follows this form:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n}) = \\mathcal{N}\\left(\\frac{1}{\\sigma^2_\\mathrm{n}}\\mathbf{\\Sigma}\\mathbf{X}^\\top\\mathbf{y}, \\mathbf{\\Sigma} \\right)\n",
    "\\end{equation}\n",
    "where $\\mathbf{\\Sigma}^{-1} = \\left(\\frac{1}{\\sigma^2_\\mathrm{n}}\\mathbf{X}^\\top\\mathbf{X} + \\mathbf{S}^{-1}  \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the dimensionality of $\\mathbf{\\Sigma}$? How much does it cost to compute that inverse? Do you know which algorithm you should use to have numerically stable results? Remember that computing $\\mathbf{A}^{-1}\\mathbf{z}$ means in practice solving a linear system.\n",
    "\n",
    "**Exercise:** Complete the following function to compute the posterior. Use the numerical sable algorithm (avoid `np.linalg.inv` on the full covariance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(X, y, sigma2priorweights, sigma2noise):\n",
    "    \n",
    "    Sigma_inverse =    # TO COMPLETE\n",
    "    posterior_mu =     # TO COMPLETE\n",
    "    posterior_Sigma =  # TO COMPLETE\n",
    "    \n",
    "    return posterior_mu, posterior_Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the posterior for the regression dataset. For the moment, place $\\sigma_\\mathrm{w}^2=1$ and start with polynomial of order 1. Finally, print the posterior mean and covariance. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2priorweights = 1\n",
    "K = 1\n",
    "\n",
    "bigX = build_X(...) # TO COMPLETE\n",
    "\n",
    "w_posterior_mu, w_posterior_Sigma = compute_posterior(...)  # TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_posterior_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_posterior_Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Visualize the prior and the posterior on a 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(-1, 5, 500)\n",
    "y_ = np.linspace(-1, 5, 500)\n",
    "X_, Y_ = np.meshgrid(x_,y_)\n",
    "pos = np.empty(X_.shape + (2,))\n",
    "pos[:, :, 0] = X_; pos[:, :, 1] = Y_\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "rv = scipy.stats.multivariate_normal(np.zeros(2), np.eye(2))\n",
    "ax.plot_surface(X_, Y_, rv.pdf(pos),cmap='viridis',linewidth=0, antialiased=False, ccount=200, rcount=200)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_zlabel('')\n",
    "ax.set_title('Prior')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "rv = scipy.stats.multivariate_normal(w_posterior_mu[...,0], w_posterior_Sigma)\n",
    "ax.plot_surface(X_, Y_, rv.pdf(pos),cmap='viridis',linewidth=0, antialiased=False, rcount=500, ccount=500)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_zlabel('')\n",
    "\n",
    "ax.set_title('Posterior')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictions\n",
    "Now it's time to make predictions. \n",
    "All our motivation for being Bayesian was to be able to average predictions at $\\mathbf{x}_\\mathrm{new}$, for all possible $\\mathbf{w}$.\n",
    "This is possible by computing the following expectation:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n})}\\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) = \\int \\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n}) \\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Prove that \n",
    "$\\mathbf{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n})}\\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) = \n",
    "\\mathcal{N}(\\mathbf{x}_\\mathrm{new}^\\top\\mathbf{\\mu}, \\sigma^2_\\mathrm{n} + \\mathbf{x}_\\mathrm{new}^\\top\\mathbf{\\Sigma}\\mathbf{x}_\\mathrm{new})$, where $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$ are the posterior mean and covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function to compute the predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictive(Xnew, w_posterior_mu, w_posterior_Sigma, sigma2noise):\n",
    "    K = w_posterior_Sigma.shape[0] - 1\n",
    "    Xnew = build_X(Xnew, K)\n",
    "    \n",
    "    y_posterior_mu =     # TO COMPLETE\n",
    "    y_posterior_Sigma = # TO COMPLETE\n",
    "    return y_posterior_mu, y_posterior_Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute and plot the predictive distribution for 100 points between -4 and +4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.linspace(-4, 4, 100)[...,None]\n",
    "y_posterior_mu, y_posterior_sigma2 = compute_predictive(...)  # TO COMPLETE\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.plot(X, y, '.k', zorder=100)\n",
    "ax.plot(Xtest, y_posterior_mu, color='C1')\n",
    "\n",
    "lb = y_posterior_mu - 3 * np.sqrt(y_posterior_sigma2)\n",
    "ub = y_posterior_mu + 3 * np.sqrt(y_posterior_sigma2)\n",
    "ax.fill_between(Xtest[...,0], lb, ub, color='C1', alpha=0.4, lw=0);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** You can also sample from the predictive posterior. \n",
    "<!-- In general, a sample from a Normal distribution $\\hat x \\sim \\mathcal{N}(x|\\mu, \\sigma^2)$ can be build as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat x = \\mu + \\varepsilon \\sigma\\,,\\quad \\mathrm{where} \\quad \\varepsilon \\sim \\mathcal{N}(0, 1)\n",
    "\\end{equation}\n",
    "\n",
    "This is known as *reparameterization trick*. Try to sample 20 times the predictive posterior and plot it. \n",
    "Try to implement yourself the reparameterization trick (`np.random.randn()` returns a sample from a standard normal with 0 mean and unit variance). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "Xtest = np.linspace(-4, 4, 100)[...,None]\n",
    "y_posterior_mu, y_posterior_sigma2 = compute_predictive(...)   # TO COMPLETE\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.plot(X, y, '.k', zorder=100)\n",
    "for _ in range(20):\n",
    "    sample_ = np.random.multivariate_normal(...)    # TO COMPLETE\n",
    "    ax.plot(Xtest, sample_, 'C1', alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try now with different polynomial order. Let's say 2, 3, 5, 10. Compute the design matrix, the posterior on $\\mathbf{w}$ and the predictive posterior. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_orders = [2, 3, 5, 10]\n",
    "Xtest = np.linspace(-4, 4, 1200)[...,None]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=[10, 6])\n",
    "axs = axs.reshape(-1)\n",
    "for i, K in enumerate(poly_orders):\n",
    "    ax = axs[i]\n",
    "    bigX = # TO COMPLETE\n",
    "    w_posterior_mu, w_posterior_Sigma =  # TO COMPLETE\n",
    "    y_posterior_mu, y_posterior_sigma2 = # TO COMPLETE\n",
    "    lb = y_posterior_mu - 3 * np.sqrt(y_posterior_sigma2)\n",
    "    ub = y_posterior_mu + 3 * np.sqrt(y_posterior_sigma2)\n",
    "    \n",
    "    ax.plot(Xtest, y_posterior_mu, color='C1')\n",
    "    ax.plot(X, y, '.k', zorder=100)\n",
    "    ax.fill_between(Xtest[...,0], lb, ub, color='C1', alpha=0.4, lw=0);\n",
    "    ax.set_ylim(-10, 20)\n",
    "    ax.set_title('Order %d' % K)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate you model: the marginal likelihood\n",
    "\n",
    "There are several ways in which you can compute the goodness of your model. The first is the likelihood itself.\n",
    "\n",
    "**Question:** Compute the mean loglikelihood for model with order from 1 to 7 and plot it. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_orders = range(1, 8)\n",
    "mll = []\n",
    "for i, K in enumerate(poly_orders):\n",
    "    bigX = build_X(X, K)\n",
    "    w_posterior_mu, w_posterior_Sigma = # TO COMPLETE\n",
    "    y_posterior_mu, _ = # TO COMPLETE\n",
    "    mll_ = # TO COMPLETE\n",
    "    mll.append(mll_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[4,2])\n",
    "ax.plot(poly_orders, mll, '-oC0')\n",
    "ax.margins(0, 0.05)\n",
    "ax.set_title('Log-Likelihood');\n",
    "ax.set_xlabel('Model (e.g. polyn. order)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Try to answer: How likely is $\\mathbf{y}$ given $\\mathbf{X}$ *and* the model (‘first/second/... order polynomial’)? Is it the same likelihood as before?\n",
    "\n",
    "So far, we’ve ignored $p(\\mathbf{y}|\\mathbf{X}, \\sigma^2_\\mathrm{n})$, the normalising thing in Bayes rule. Being a normalization constant, it has to be equal to \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|\\mathbf{X}, \\sigma^2_\\mathrm{n}) = \\int p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}, \\sigma^2_\\mathrm{n})\n",
    "p(\\mathbf{w})\\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "We’re averaging over all values of $\\mathbf{w}$ to get a value for how good the model is.\n",
    "\n",
    "**Question:** Suppose the prior being $\\mathcal{N}(\\mu_0, \\mathbf{\\Sigma}_0)$ and the likelihood $\\mathcal{N}(\\mathbf{X}\\mathbf{w}, \\sigma^2_\\mathrm{n} \\mathbf{I})$. Derive the marginal likelihood (hint: don't solve the integral -- check the rules for Gaussian conditioning and marginalization) (big hint: check the lecture notes).\n",
    "\n",
    "**Exercise:** Write a function to compute the marginal likelihood. Remember: this is a *likelihood* not a density. You should return a number not a density. For simplicity, assume $\\mu_0 = 0$ and $\\Sigma_0 = \\sigma^2_\\mathrm{w}\\mathbf{I}$. Use `scipy.stats.multivariate_normal` for computing the logpdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def marginal_likelihood(X, y, sigma2w, sigma2noise):\n",
    "    return # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Do the sample plot as before, but now plot the marginal likelihood. You should see a clear pattern here; comment the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "poly_orders = range(1, 8)\n",
    "mll = []\n",
    "for i, K in enumerate(poly_orders):\n",
    "    bigX =   # TO COMPLETE\n",
    "    mll_ =   # TO COMPLETE\n",
    "    mll.append(mll_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5,2])\n",
    "ax.plot(poly_orders, mll, '-oC0')\n",
    "ax.margins(0, 0.05)\n",
    "ax.set_title('Log-Marginal Likelihood');\n",
    "ax.set_xlabel('Model (e.g. polyn. order)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. When polynomial features are not enough (++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, zipfile, io\n",
    "from shutil import copyfile, rmtree\n",
    "def snelson1d(path):\n",
    "    path = os.path.expanduser(path)\n",
    "    inputs_path = os.path.join(path, 'snelson_train_inputs')\n",
    "    outputs_path = os.path.join(path, 'snelson_train_outputs')\n",
    "\n",
    "    # Contains all source as well. We just need the data.\n",
    "    url = 'http://www.gatsby.ucl.ac.uk/~snelson/SPGP_dist.zip'\n",
    "\n",
    "    if not (os.path.exists(inputs_path) and os.path.exists(outputs_path)):\n",
    "        r = requests.get(url)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(path)\n",
    "\n",
    "        # Copy the required data\n",
    "        copyfile(os.path.join(path, \"SPGP_dist\", \"train_inputs\"), inputs_path)\n",
    "        copyfile(os.path.join(path, \"SPGP_dist\", \"train_outputs\"), outputs_path)\n",
    "\n",
    "        # Clean up everything else\n",
    "        rmtree(os.path.join(path, \"SPGP_dist\"))\n",
    "    \n",
    "\n",
    "    X = np.loadtxt(os.path.join(inputs_path))[:, None]\n",
    "    Y = np.loadtxt(os.path.join(outputs_path))[:, None]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, Y = snelson1d('./data')\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6, 4])\n",
    "ax.plot(X , Y, 'k.')\n",
    "ax.set_title('Snelson dataset')\n",
    "ax.set_xlim(-3, 9)\n",
    "ax.set_ylim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from looking at the dataset, you can imagine that fitting this dataset will be hard. And it is (with polynomial features).\n",
    "Maybe it becomes easier with different type of features.\n",
    "Remember that you can choose whatever you want to build the design matrix. Take a look at the following \n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\sqrt{\\frac{\\alpha^2}{K}}\\left[ \n",
    "    {\\begin{array}{cccccc}\n",
    "         \\cos(\\omega_1x_1) &  \\sin(\\omega_1x_1) & \\dots &   \\cos(\\omega_Kx_1) & \\sin(\\omega_Kx_1) \\\\\n",
    "         \\cos(\\omega_1x_2) &  \\sin(\\omega_1x_2) & \\dots &   \\cos(\\omega_Kx_2) & \\sin(\\omega_Kx_2) \\\\\n",
    "         \\vdots &  \\vdots &       &   \\vdots  \\\\\n",
    "          \\cos(\\omega_1x_N) &  \\sin(\\omega_1x_N) & \\dots &   \\cos(\\omega_Kx_N) & \\sin(\\omega_Kx_N) \\\\\n",
    "  \\end{array} } \n",
    "\\right]\n",
    "\\quad\n",
    "\\mathrm{where}\n",
    "\\quad \n",
    "\\omega_i \\sim \\mathcal N (0, \\lambda)\n",
    "$$\n",
    "\n",
    "Now, the next formulation of the design matrix $\\mathbf{X}$ might seems to come completely out of the blue, but it's not (for those of you interested, this is the random feature expansion of the RBF kernel -- join the next lecture to know more).\n",
    "\n",
    "For simplicity below you have the code to compute $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now run Bayesian linear regression on this dataset with this new set of features (NB. You might need many MANY features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "omega = np.random.randn(10000)/.5\n",
    "def build_X(X: np.ndarray, K: int):\n",
    "    assert K >= 0 and isinstance(K, int)\n",
    "    \n",
    "    X_ =  np.column_stack([np.sin(X * omega[k]) for k in range(K//2+1)])\n",
    "    Z_ =  np.column_stack([np.cos(X * omega[k]) for k in range(K//2)])\n",
    "    X_ =  np.concatenate([X_, Z_], -1) \n",
    "    return X_ * np.sqrt(1/K)\n",
    "\n",
    "\n",
    "Xtest = np.linspace(X.min()-3, X.max()+5, 500)\n",
    "n_features = [1*2, 5*2, 100*2, 2500*2]\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=[6, 12])\n",
    "axs = axs.reshape(-1)\n",
    "for i, K in enumerate(n_features):\n",
    "    ax = axs[i]\n",
    "   \n",
    "    bigX = build_X(X, K)\n",
    "\n",
    "\n",
    "    w_posterior_mu, w_posterior_Sigma = # TO COMPLETE\n",
    "    # TO COMPLETE\n",
    "    \n",
    "    ax.plot(X , Y, 'k.')\n",
    "    # TO COMPLETE\n",
    " \n",
    "    ax.set_title('Regression with %d random features' % (K//2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. A more complex model: being Bayesian on the noise (++)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready spice things up! Everything we did, we did it assuming that the noise variance $\\sigma_\\mathrm{n}^2$ was known. This is a far too restrictive assumption in practice. \n",
    "There are several ways to choose $\\sigma_\\mathrm{n}^2$ (cross-validation, maximization of the marginal likelihood w.r.t. $\\sigma_\\mathrm{n}^2$, ...).\n",
    "But this -- being a course on Bayesian inference -- requires to develop a solution in a \"Bayesian\" way: place a prior on $\\sigma_\\mathrm{n}^2$ and infer a posterior given some data.\n",
    "\n",
    "As usual, the likelihood has the form $p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\mathcal{N}(\\mathbf{X}\\mathbf{w}, \\sigma^2_\\mathrm{n} \\mathbf{I})$.\n",
    "\n",
    "One can show that the natural conjugate prior is a normal-inverse Gaussian distribution with the following form:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{w}, \\sigma^2_\\mathrm{n})  &= \\mathrm{NIG}(\\mathbf{w}, \\sigma^2_\\mathrm{n}|\\mu_0, \\mathbf{\\Sigma}_0, a_0, b_0) = \\\\\n",
    "                                    &= \\mathcal{N}(\\mathbf{w}|\\mu_0,\\sigma^2_\\mathrm{n}\\mathbf{\\Sigma}_0)\\mathrm{IG}(\\sigma^2_\\mathrm{n}|a_0, b_0)\n",
    "\\end{align}\n",
    "\n",
    "where IG is the inverse Gamma distribution.\n",
    "\n",
    "**Exercise:** Since changes are you never saw an inverse Gamma distribution, use the following cell to play with it. Try to change the parameters $a_0$ and $b_0$. What happens if you both set them to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0, b0 = 1, 3\n",
    "rv = scipy.stats.invgamma(a=a0, scale=b0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[4, 2.5])\n",
    "xplot = np.linspace(0, 10, 100)\n",
    "ax.plot(xplot, rv.pdf(xplot))\n",
    "ax.set_title('IG(%.1f,%.1f)' % (a0,b0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can derive the posterior $p(\\mathbf{w},\\sigma_\\mathrm{n}^2 | \\mathbf{X}, \\mathbf{y})$ as follows\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{w},\\sigma_\\mathrm{n}^2 | \\mathbf{X}, \\mathbf{y}) &= \\mathrm{NIG}(\\mathbf{w},\\sigma_\\mathrm{n}^2|\\mu, \\mathbf{\\Sigma}, a, b)\\\\\n",
    "\\mathbf{\\Sigma} &= (\\mathbf{\\Sigma}_0^{-1} + \\mathbf{X}^\\top\\mathbf{X})^{-1}\\\\\n",
    "\\mu &= \\mathbf{\\Sigma}(\\mathbf{\\Sigma}_0^{-1}\\mu_0 + \\mathbf{X}^\\top\\mathbf{y}) \\\\\n",
    "a &= a_0 + n/2 \\\\ \n",
    "b &= b_0 + \\frac{1}{2}(\\mu_0^\\top\\mathbf{\\Sigma}_0\\mu_0 + \\mathbf{y}^\\top\\mathbf{y} - \\mu^\\top\\mathbf{\\Sigma}\\mu )\n",
    "\\end{align}\n",
    "\n",
    "This is left as an exercise to the reader (joking aside if you want to know more check Sec. 7.6.3.1 of \"Machine Learning: A probabilistic perspective\" by K.P. Murphy).\n",
    "From this formulation, you can derive the two marginals on $\\mathbf{w}$ and ${\\sigma_\\mathrm{n}^2}$, which are easier to understand.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\sigma_\\mathrm{n}^2|\\mathrm{X}, \\mathrm{y}) &= \\mathrm{IG}(a,b)\\\\\n",
    "p(\\mathbf{w}|\\mathrm{X}, \\mathrm{y}) &= \\mathcal{T}\\left(\\mu, \\frac{b}{a}\\Sigma, 2a\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal{T}$ is a student-T distribution. \n",
    "\n",
    "Finally, the predictive distribution a new test points $\\mathbf{X}_{\\mathrm{new}}$ is again a student-T distribution with the following form, \n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{y}_\\mathrm{new}| \\mathbf{X}_\\mathrm{new}, \\mathbf{X}, \\mathbf{y}) = \\mathcal{T}\\left(\\mathbf{X}_\\mathrm{new}\\mu, \\frac{b}{a}(I + \\mathbf{X}_\\mathrm{new}\\mathbf{\\Sigma}\\mathbf{X}_\\mathrm{new}^\\top), 2a \\right)\n",
    "\\end{align}\n",
    "\n",
    "**Exercise:** Write the function to compute the posterior following the formula above. Compute also the predictive posterior and plot few samples from it. Start with order 1. \n",
    "<!-- NB: A sample from the student-T distribution can be constructed using the *reparameterization trick* similarly to the Gaussian,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat x = \\mu + \\varepsilon \\sigma\\,,\\quad \\mathrm{where} \\quad \\varepsilon \\sim \\mathcal{T}(0, 1, \\mathrm{dof})\n",
    "\\end{equation} -->\n",
    "\n",
    "For the prior, choose $\\mu_0 = 0$, $\\mathbf{\\Sigma}_0 = \\mathbf{I}$ and $a_0 = b_0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "sigma2noise = .1\n",
    "X, y = make_regression(20, sigma2noise=sigma2noise)\n",
    "Xtest = np.linspace(-4, 4, 100)[...,None]\n",
    "\n",
    "def build_X(X: np.ndarray, K: int):\n",
    "    return np.column_stack([X ** k for k in range(K+1)])\n",
    "\n",
    "def compute_posterior(X, y, mu0, Sigma0, a0, b0):\n",
    "    Sigma = # TO COMPLETE\n",
    "    mu =    # TO COMPLETE \n",
    "    a =     # TO COMPLETE\n",
    "    b =     # TO COMPLETE\n",
    "    \n",
    "    return mu, Sigma, a, b\n",
    "\n",
    "def compute_predictive_posterior(Xnew, mu, Sigma, a, b):\n",
    "    Xnew = build_X(Xnew, len(mu)-1)\n",
    "    m = # TO COMPLETE\n",
    "    c = # TO COMPLETE\n",
    "    dof = # TO COMPLETE\n",
    "    return m, np.diag(c), dof\n",
    "\n",
    "def sample_from_student_t(m, v, dof, n):\n",
    "    d = len(m)\n",
    "    if dof == np.inf:\n",
    "        x = 1.\n",
    "    else:\n",
    "        x = np.random.chisquare(dof, n)/dof\n",
    "    z = np.random.multivariate_normal(np.zeros(d),S,(n,))\n",
    "    return m + z/np.sqrt(x)[:,None]   # same output format as random.multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "K = 1\n",
    "bigX = # TO COMPLETE\n",
    "mu0 = # TO COMPLETE\n",
    "Sigma0 = # TO COMPLETE\n",
    "a0, b0 = 0, 0\n",
    "\n",
    "mu, Sigma, a, b = # TO COMPLETE\n",
    "\n",
    "m, v, dof = # TO COMPLETE\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    " # TO COMPLETE\n",
    "ax.plot(X, y, '.k', zorder=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compare this plot with the previous one with order 1. Do you see any difference? Comment the results.\n",
    "\n",
    "**Exercise:** Do the same a before, but now with order 2, 3, 5 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "poly_orders = [2, 3, 5, 10]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=[10, 6])\n",
    "axs = axs.reshape(-1)\n",
    "for i, K in enumerate(poly_orders):\n",
    "    ax = axs[i]\n",
    "    bigX = # TO COMPLETE\n",
    "    mu0 = # TO COMPLETE\n",
    "    Sigma0 = # TO COMPLETE\n",
    "    a0, b0 = 0, 0\n",
    "    mu, Sigma, a, b = # TO COMPLETE\n",
    "    m, v, dof = # TO COMPLETE\n",
    "     # TO COMPLETE\n",
    "    ax.set_ylim(-10, 20)\n",
    "    ax.plot(X, y, '.k', zorder=100)\n",
    "    ax.set_title('Order %d' % K)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
